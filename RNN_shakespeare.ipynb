{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Untitled9.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbUyEL-EEGn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN _  many to many (equal size)\n",
        "#I used teacher forcing and didn't consider schadule sampling, for more robust results, you should implement schadule sampling! \n",
        "#import packages\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1476pwvEJjX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6cd8ba50-edab-4694-da63-00191a1c4387"
      },
      "source": [
        "#getting datase with tensorflow\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdMOTSxTEL4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO-F8VhJEU2q",
        "colab_type": "code",
        "outputId": "1309afd3-1e9a-4616-db6c-ec1c9df85c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "print(\"Number of words in the corpus: \",len(corpus))\n",
        "text=corpus.splitlines()\n",
        "print(\"Number of lines in the corpus: \",len(text))\n",
        "\n",
        "print(\"The first line of corpus is => \",text[0])\n",
        "# create train, test and validation set\n",
        "for i in range(len(text)):\n",
        "  text[i]=text[i]+\" <end>\" ## adding <end> at end of each line \n",
        "print(\"Now the first line of corpus is => \",text[0]) \n",
        "\n",
        "\n",
        "words=' '.join(text)\n",
        "words=words.split()\n",
        "print(\"printing the 0th to 100th  words of corpus : \",words[0:100])\n",
        "## later, we use these words for creating vocabulary \n",
        "## we dont consider spaces and new lines!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the corpus:  1115394\n",
            "Number of lines in the corpus:  40000\n",
            "The first line of corpus is =>  First Citizen:\n",
            "Now the first line of corpus is =>  First Citizen: <end>\n",
            "printing the 0th to 100th  words of corpus :  ['First', 'Citizen:', '<end>', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', '<end>', '<end>', 'All:', '<end>', 'Speak,', 'speak.', '<end>', '<end>', 'First', 'Citizen:', '<end>', 'You', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish?', '<end>', '<end>', 'All:', '<end>', 'Resolved.', 'resolved.', '<end>', '<end>', 'First', 'Citizen:', '<end>', 'First,', 'you', 'know', 'Caius', 'Marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people.', '<end>', '<end>', 'All:', '<end>', 'We', \"know't,\", 'we', \"know't.\", '<end>', '<end>', 'First', 'Citizen:', '<end>', 'Let', 'us', 'kill', 'him,', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', 'price.', '<end>', \"Is't\", 'a', 'verdict?', '<end>', '<end>', 'All:', '<end>', 'No', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done:', 'away,', 'away!', '<end>', '<end>', 'Second']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ga3Wtuq0xMx",
        "colab_type": "code",
        "outputId": "b20975bb-7f73-410e-e52a-d2389473c602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#create vocabulary\n",
        "\n",
        "voc=[]\n",
        "\n",
        "def return_index(word):\n",
        "  return voc.index(word)\n",
        " \n",
        "for word in words :\n",
        "  if word not in voc:\n",
        "    voc.append(word)\n",
        "    \n",
        "print(\"The index of  word (The) in vocabulary: \",return_index('The') )\n",
        "print(\"The word for index (203): \",voc[203] )\n",
        "\n",
        "print(\"Vocab lenght: \",len(voc))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The index of  word (The) in vocabulary:  203\n",
            "The word for index (203):  The\n",
            "Vocab lenght:  25671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJxYsGfx7OIT",
        "colab_type": "code",
        "outputId": "a37f8f57-75c3-4d57-e486-e9ddffb7371c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# converting words to indexes and save it in words_indexes\n",
        "words_indexes=[return_index(word) for word in words]\n",
        "print(\"printing some first indexes : \",words_indexes[0:100])\n",
        "## its our network input! we use words_indexes  to feed the network\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing some first indexes :  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 11, 2, 12, 10, 2, 2, 0, 1, 2, 13, 14, 15, 16, 17, 18, 19, 20, 18, 21, 2, 2, 11, 2, 22, 23, 2, 2, 0, 1, 2, 24, 25, 26, 27, 28, 29, 30, 31, 18, 32, 33, 2, 2, 11, 2, 34, 35, 4, 36, 2, 2, 0, 1, 2, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 2, 49, 50, 51, 2, 2, 11, 2, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 2, 2, 62]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cjjZtu9-0o0",
        "colab_type": "code",
        "outputId": "6d06c423-c176-4906-a6ee-999a7c8a2c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "seq_lenght=35 # the lenght of rnn (the lenght of input)\n",
        "batch_size=20\n",
        "# how many batches(with size: [batch_size, seq_lenght])  we have for words_indexes array ?\n",
        "# the answer is len(words_indexes/(batch_size*seq_lenght))\n",
        "max_batches=int(len(words_indexes)/(batch_size*seq_lenght))\n",
        "print(\"the maximum numbers of batcheswith these setting(batch_size,seq_lenght) :  \",max_batches)\n",
        "\n",
        "\n",
        "\n",
        "trainsetX=words_indexes[0:int(0.9*max_batches)*batch_size*seq_lenght]\n",
        "trainsetY=words_indexes[1:int(0.9*max_batches)*batch_size*seq_lenght+1]\n",
        "validsetX=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght:int(1*max_batches)*batch_size*seq_lenght]\n",
        "validsetY=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght + 1:int(1*max_batches)*batch_size*seq_lenght +1]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the maximum numbers of batcheswith these setting(batch_size,seq_lenght) :   346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJqJCYHp0x_0",
        "colab_type": "code",
        "outputId": "36c38f65-ed48-4288-b26e-af95dba1cbde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "##Creating batches\n",
        "\n",
        "def create_batch(data,flag):\n",
        "  if flag=='Train':\n",
        "     return (np.array(data).reshape(int(0.9*max_batches) ,batch_size ,seq_lenght)) # shape(numer of batch, batch_size,seq_enght)\n",
        "  else :\n",
        "     return (np.array(data).reshape(int(1*max_batches)-int(0.9*max_batches) ,batch_size ,seq_lenght)) # shape(numer of batch, batch_size,seq_enght)\n",
        "  \n",
        "trainX_batches =create_batch(trainsetX,'Train')\n",
        "trainY_batches =create_batch(trainsetY,'Train')\n",
        "\n",
        "validX_batches =create_batch(validsetX,'Valid')\n",
        "validY_batches =create_batch(validsetY,'Valid')\n",
        "\n",
        "\n",
        "print(\"trainX_batches_shape: \",trainX_batches.shape)\n",
        "print(\"trainY_batches_shape: \",trainY_batches.shape)\n",
        "\n",
        "print(\"validX_batches_shape: \",validX_batches.shape)\n",
        "print(\"validY_batches_shape: \",validY_batches.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainX_batches_shape:  (311, 20, 35)\n",
            "trainY_batches_shape:  (311, 20, 35)\n",
            "validX_batches_shape:  (35, 20, 35)\n",
            "validY_batches_shape:  (35, 20, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaCR9Gi0DEv0",
        "colab_type": "code",
        "outputId": "895b4fe6-e80f-44d4-c6b4-e735b72b509c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "print(trainX_batches[0,0,:]) # batchNumber=0 , 0th sample in the batch , all of the seq lenght\n",
        "print(trainY_batches[0,0,:])\n",
        "print(len(words_indexes))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10  2  2 11  2 12 10  2  2  0  1  2 13 14\n",
            " 15 16 17 18 19 20 18 21  2  2 11]\n",
            "[ 1  2  3  4  5  6  7  8  9 10  2  2 11  2 12 10  2  2  0  1  2 13 14 15\n",
            " 16 17 18 19 20 18 21  2  2 11  2]\n",
            "242651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBT0GZrS1IoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now pack these information into one function\n",
        "def get_batches():# use in training \n",
        "  global  words_indexes\n",
        "  a=[]\n",
        "  b=[]\n",
        "    ## shuffling  words_indexes\n",
        "  a=words_indexes[0:int(0.2*len(words_indexes))]\n",
        "  b=words_indexes[int(0.2*len(words_indexes)):]\n",
        "  words_indexes=b+a\n",
        "\n",
        "  trainsetX=words_indexes[0:int(0.9*max_batches)*batch_size*seq_lenght]\n",
        "  trainsetY=words_indexes[1:int(0.9*max_batches)*batch_size*seq_lenght+1]\n",
        "  validsetX=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght:int(1*max_batches)*batch_size*seq_lenght]\n",
        "  validsetY=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght + 1:int(1*max_batches)*batch_size*seq_lenght +1]\n",
        "\n",
        "  trainX_batches =create_batch(trainsetX,'Train')\n",
        "  trainY_batches =create_batch(trainsetY,'Train')\n",
        "  validX_batches =create_batch(validsetX,'Valid')\n",
        "  validY_batches =create_batch(validsetY,'Valid')\n",
        "  \n",
        "  return  trainX_batches,trainY_batches,validX_batches,validY_batches\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gPNo1yy1I2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a network\n",
        "\n",
        "class net(torch.nn.Module):\n",
        "  def  __init__(self,vocab_size,embed_size,hidden_size,number_of_layers,num_dir):\n",
        "    super().__init__()\n",
        "    \n",
        "\n",
        "  \n",
        "    #If the RNN is bidirectional,num_directions should be 2, else it should be 1.\n",
        "  \n",
        "    self.num_directions=num_dir\n",
        "    self.number_of_layers=number_of_layers\n",
        "    self.hidden_size=hidden_size\n",
        "\n",
        "    \n",
        "    \n",
        "    self.embd= torch.nn.Embedding(vocab_size, embed_size)\n",
        "    self.drop1=torch.nn.Dropout(0.3)\n",
        "    self.gru = torch.nn.GRU(embed_size, hidden_size, number_of_layers, dropout=0.4,bidirectional=False)\n",
        "    self.drop2=torch.nn.Dropout(0.1)\n",
        "    self.fc=torch.nn.Linear(hidden_size,vocab_size)\n",
        "    self.init_weights()\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "  def forward(self,x,hidden):\n",
        "    # x shape : torch.Size([seq_lenght, batch_size]) contains index of words in vocab\n",
        "    # hidden shape :torch.Size([num_layers * num_directions, batch_size, hidden_size])\n",
        "    # hidden means h0\n",
        "    \n",
        "    \n",
        "    emb=self.embd(x) \n",
        "    # emb shape : torch.Size([seq_lenght, batch_size, embed_size])\n",
        "    out=self.drop1(emb)\n",
        "    out , hidden =self.gru(emb,hidden) #  out = the toppest  hidden layer for all time steps |  hidden= contains hidden of all hidden layers of last time step\n",
        "    # out shape :  torch.Size([seq_lenght, batch_size, hidden_size  * num_directions])\n",
        "    # hidden shape : torch.Size([num_layers * num_directions, batch_size, hidden_size])\n",
        "    \n",
        "    \n",
        "    out=self.drop2(out) # it doesnt  change the dimension\n",
        "   \n",
        "    output=out.view(out.size(0)* out.size(1),out.size(2))   \n",
        "    # out shape:  torch.Size([seq_lenght * batch_size, hidden_size  * num_directions])\n",
        "    \n",
        "    \n",
        "    output=self.fc(output)\n",
        "    # output shape:  torch.Size([seq_lenght * batch_size, vocab_size])\n",
        "    \n",
        "    \n",
        "    output=output.view(out.shape[0], out.shape[1],output.shape[1])   \n",
        "    # output shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "\n",
        "    \n",
        "    # hidden is h0 for next feeding\n",
        "    return output , hidden\n",
        "  def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embd.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "  def init_hidden(self, batch_size): # initialization with zeroes\n",
        "        weight = next(self.parameters()).data\n",
        "        return torch.autograd.Variable(weight.new(self.number_of_layers, batch_size, self.hidden_size).zero_())\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stvpINrJ1JGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(voc)\n",
        "embed_size=250\n",
        "hidden_size=200\n",
        "number_of_layers=2\n",
        "num_direction=1\n",
        "learning_rate=0.001\n",
        "clip = 0.25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AANGMmnTQjw0",
        "colab_type": "code",
        "outputId": "459a0f4c-63ad-4886-aa46-7f23b1dc3bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "model=net(vocab_size,embed_size,hidden_size,number_of_layers,num_direction)\n",
        "print(model)\n",
        "if cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net(\n",
            "  (embd): Embedding(25671, 250)\n",
            "  (drop1): Dropout(p=0.3)\n",
            "  (gru): GRU(250, 200, num_layers=2, dropout=0.4)\n",
            "  (drop2): Dropout(p=0.1)\n",
            "  (fc): Linear(in_features=200, out_features=25671, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTgsiNIyQls0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtQ1I_Kw5HHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculating accuracy\n",
        "\n",
        "def cal_accuracy(pred_classes,real_classes):#  shape -> both : [number of test samples,1]  or both : [number of test samples,]\n",
        "  bool_array=(pred_classes==real_classes) # example: bool_array=[True, False, True, False, True]\n",
        "  True_pred_counts=np.count_nonzero(bool_array) # count number of Trues in [True, False, True, False, True]\n",
        "  \n",
        "  return True_pred_counts/pred_classes.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap8AXWsIV1ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# important notices:\n",
        "# a.shape=(2,3,4)\n",
        "# a.rehspae(6,4) -> (a000) (a001) (a002) (a003)      ,a021=a[0,2,1]\n",
        "#                   (a010) (a011) (a012) (a013) \n",
        "#                   (a020) (a021) (a022) (a023) \n",
        "#                   (a100) (a101) (a102) (a103) \n",
        "#                   (a110) (a111) (a112) (a113) \n",
        "#                   (a120) (a121) (a122) (a123) \n",
        "#\n",
        "#\n",
        "# a.shape=(3,2,4)\n",
        "# a.rehspae(6,4) -> (a000) (a001) (a002) (a003) \n",
        "#                   (a010) (a011) (a012) (a013) \n",
        "#                   (a100) (a101) (a102) (a103) \n",
        "#                   (a110) (a111) (a112) (a113) \n",
        "#                   (a200) (a201) (a202) (a203) \n",
        "#                   (a210) (a211) (a212) (a213) \n",
        "#\n",
        "#\n",
        "# a.shape=(3,2)\n",
        "# a.rehspae(6,1) -> (a00) \n",
        "#                   (a01)  \n",
        "#                   (a10)                   \n",
        "#                   (a11)  \n",
        "#                   (a20)  \n",
        "#                   (a21) \n",
        "#\n",
        "#\n",
        "# a.shape=(2,3)\n",
        "# a.rehspae(6,1) -> (a00) \n",
        "#                   (a01)  \n",
        "#                   (a02)                   \n",
        "#                   (a10)  \n",
        "#                   (a11)  \n",
        "#                   (a12) \n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a0rmFKiQmXG",
        "colab_type": "code",
        "outputId": "07b92aea-98eb-4956-d6bd-3c6cba517cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs=300\n",
        "mean_train_acc=[]\n",
        "mean_valid_acc=[]\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  validacc=[]\n",
        "  trainacc=[]\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  total_loss=0\n",
        "  model.train()\n",
        "  trainX_batches,trainY_batches,_,_=get_batches()\n",
        "  for batchNumber in range(trainX_batches.shape[0]):\n",
        "    \n",
        "    X=torch.autograd.Variable(torch.LongTensor(trainX_batches[batchNumber].T)) #train_batches[batchNumber].T  shape :nparray ([seq_lenght,batch_size]) \n",
        "    Y=torch.autograd.Variable(torch.LongTensor(trainY_batches[batchNumber].T.reshape(seq_lenght*batch_size)))  #Y:Torch.szie ([seq_lenght*batch_size]) => seq0batch0  ,  seq0batch1 ,  seq0batch2  , ..... \n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "            Y = Y.cuda()\n",
        "     \n",
        "\n",
        "    hidden = torch.autograd.Variable(hidden)#seperating  hidden state from one iteration to another\n",
        "    # because we want to use this new hidden for computational graph, not preivous hiddens\n",
        "   \n",
        "    if cuda.is_available():\n",
        "             hidden = hidden.cuda()\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs,hidden=model(X,hidden)# outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(seq_lenght*batch_size, vocab_size)\n",
        "    # outputs shape:  torch.Size([seq_lenght * batch_size, vocab_size])=> seq0batch0,vocab  ,  seq0batch1,vocab ,  seq0batch2,vocab  , .....\n",
        "    \n",
        "    loss=criterion(outputs,Y)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    pred=outputs\n",
        "    pred_classes=torch.max(pred, 1)[1]# pred_classes.shape : torch.Size([seq_lenght * batch_size, vacab_size])\n",
        "    acc=cal_accuracy(np.array(pred_classes.cpu()),Y.cpu().numpy()) \n",
        "    trainacc.append(acc*100)\n",
        "\n",
        "  print(\"training accuracy in epoch: \",i,\" is \",np.mean(trainacc))\n",
        "  mean_train_acc.append(np.mean(trainacc))\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  model.eval() \n",
        "  _,_,validX_batches,validY_batches=get_batches()\n",
        "\n",
        "  for batchNumber in range(validX_batches.shape[0]):\n",
        "    \n",
        "    X=torch.autograd.Variable(torch.LongTensor(validX_batches[batchNumber].T)) #valid_batches[batchNumber].T  shape :nparray ([seq_lenght,batch_size]) \n",
        "    Y=torch.autograd.Variable(torch.LongTensor(validY_batches[batchNumber].T.reshape(seq_lenght*batch_size)))  #Y:Torch.szie ([seq_lenght*batch_size]) \n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "            Y = Y.cuda()\n",
        "  \n",
        "    if cuda.is_available():\n",
        "            hidden = hidden.cuda()\n",
        "      \n",
        "    outputs,hidden=model(X,hidden)# outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(seq_lenght*batch_size, vocab_size)\n",
        "    # outputs shape:  torch.Size([seq_lenght * batch_size, vocab_size])\n",
        "    pred=outputs \n",
        "    pred_classes=torch.max(pred.data, 1)[1]# torch.Size([seq_lenght * batch_size, vacab_size])\n",
        "    acc=cal_accuracy(np.array(pred_classes.cpu()),Y.cpu().numpy())\n",
        "    validacc.append(acc*100)\n",
        "\n",
        "  print(\"validation accuracy in epoch: \",i,\" is \",np.mean(validacc))\n",
        "  mean_valid_acc.append(np.mean(validacc))\n",
        "\n",
        "  "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training accuracy in epoch:  0  is  16.58337161231052\n",
            "validation accuracy in epoch:  0  is  16.346938775510207\n",
            "training accuracy in epoch:  1  is  18.8226917776757\n",
            "validation accuracy in epoch:  1  is  19.1265306122449\n",
            "training accuracy in epoch:  2  is  20.698208543867707\n",
            "validation accuracy in epoch:  2  is  21.057142857142857\n",
            "training accuracy in epoch:  3  is  22.294441892512634\n",
            "validation accuracy in epoch:  3  is  22.03673469387755\n",
            "training accuracy in epoch:  4  is  23.227377124483233\n",
            "validation accuracy in epoch:  4  is  23.771428571428576\n",
            "training accuracy in epoch:  5  is  23.815342214056038\n",
            "validation accuracy in epoch:  5  is  22.31836734693878\n",
            "training accuracy in epoch:  6  is  24.366559485530548\n",
            "validation accuracy in epoch:  6  is  22.877551020408166\n",
            "training accuracy in epoch:  7  is  24.79007808911346\n",
            "validation accuracy in epoch:  7  is  24.069387755102046\n",
            "training accuracy in epoch:  8  is  25.418925126320623\n",
            "validation accuracy in epoch:  8  is  25.30612244897959\n",
            "training accuracy in epoch:  9  is  26.08406063389986\n",
            "validation accuracy in epoch:  9  is  25.75918367346939\n",
            "training accuracy in epoch:  10  is  26.957740009186956\n",
            "validation accuracy in epoch:  10  is  23.689795918367345\n",
            "training accuracy in epoch:  11  is  27.60725769407441\n",
            "validation accuracy in epoch:  11  is  25.240816326530613\n",
            "training accuracy in epoch:  12  is  28.373449701423976\n",
            "validation accuracy in epoch:  12  is  26.02857142857143\n",
            "training accuracy in epoch:  13  is  29.409738171796054\n",
            "validation accuracy in epoch:  13  is  27.404081632653064\n",
            "training accuracy in epoch:  14  is  30.632062471290762\n",
            "validation accuracy in epoch:  14  is  28.057142857142853\n",
            "training accuracy in epoch:  15  is  31.66146072576941\n",
            "validation accuracy in epoch:  15  is  25.669387755102036\n",
            "training accuracy in epoch:  16  is  32.43362425355995\n",
            "validation accuracy in epoch:  16  is  27.044897959183675\n",
            "training accuracy in epoch:  17  is  33.42214056040422\n",
            "validation accuracy in epoch:  17  is  29.29387755102041\n",
            "training accuracy in epoch:  18  is  34.59209921910887\n",
            "validation accuracy in epoch:  18  is  28.89795918367347\n",
            "training accuracy in epoch:  19  is  35.80569591180524\n",
            "validation accuracy in epoch:  19  is  30.546938775510203\n",
            "training accuracy in epoch:  20  is  36.71658245291687\n",
            "validation accuracy in epoch:  20  is  30.55510204081633\n",
            "training accuracy in epoch:  21  is  37.446945337620576\n",
            "validation accuracy in epoch:  21  is  28.8530612244898\n",
            "training accuracy in epoch:  22  is  38.0303169499311\n",
            "validation accuracy in epoch:  22  is  31.53877551020409\n",
            "training accuracy in epoch:  23  is  39.49563619660083\n",
            "validation accuracy in epoch:  23  is  32.56734693877551\n",
            "training accuracy in epoch:  24  is  40.21956821313734\n",
            "validation accuracy in epoch:  24  is  33.30612244897959\n",
            "training accuracy in epoch:  25  is  41.152962792834174\n",
            "validation accuracy in epoch:  25  is  34.05714285714286\n",
            "training accuracy in epoch:  26  is  41.80064308681672\n",
            "validation accuracy in epoch:  26  is  32.24081632653061\n",
            "training accuracy in epoch:  27  is  42.24712907671107\n",
            "validation accuracy in epoch:  27  is  35.64489795918367\n",
            "training accuracy in epoch:  28  is  43.30730362884703\n",
            "validation accuracy in epoch:  28  is  34.42857142857144\n",
            "training accuracy in epoch:  29  is  44.20670647680294\n",
            "validation accuracy in epoch:  29  is  36.64489795918367\n",
            "training accuracy in epoch:  30  is  45.079926504363804\n",
            "validation accuracy in epoch:  30  is  36.41632653061224\n",
            "training accuracy in epoch:  31  is  45.32338079926504\n",
            "validation accuracy in epoch:  31  is  35.302040816326524\n",
            "training accuracy in epoch:  32  is  45.605420303169495\n",
            "validation accuracy in epoch:  32  is  38.77142857142857\n",
            "training accuracy in epoch:  33  is  46.7220946256316\n",
            "validation accuracy in epoch:  33  is  39.59591836734693\n",
            "training accuracy in epoch:  34  is  47.39366100137804\n",
            "validation accuracy in epoch:  34  is  40.61224489795918\n",
            "training accuracy in epoch:  35  is  48.162609095084974\n",
            "validation accuracy in epoch:  35  is  41.56734693877551\n",
            "training accuracy in epoch:  36  is  48.410656867248505\n",
            "validation accuracy in epoch:  36  is  37.94285714285714\n",
            "training accuracy in epoch:  37  is  48.67386311437759\n",
            "validation accuracy in epoch:  37  is  43.44489795918367\n",
            "training accuracy in epoch:  38  is  49.53468075333027\n",
            "validation accuracy in epoch:  38  is  40.526530612244905\n",
            "training accuracy in epoch:  39  is  50.18190169958659\n",
            "validation accuracy in epoch:  39  is  42.06122448979592\n",
            "training accuracy in epoch:  40  is  50.913642627468995\n",
            "validation accuracy in epoch:  40  is  42.036734693877555\n",
            "training accuracy in epoch:  41  is  51.06201194304088\n",
            "validation accuracy in epoch:  41  is  41.546938775510206\n",
            "training accuracy in epoch:  42  is  51.52365640790078\n",
            "validation accuracy in epoch:  42  is  44.6530612244898\n",
            "training accuracy in epoch:  43  is  52.19935691318329\n",
            "validation accuracy in epoch:  43  is  43.457142857142856\n",
            "training accuracy in epoch:  44  is  52.78226917776757\n",
            "validation accuracy in epoch:  44  is  45.24081632653061\n",
            "training accuracy in epoch:  45  is  53.49655489205329\n",
            "validation accuracy in epoch:  45  is  44.8530612244898\n",
            "training accuracy in epoch:  46  is  53.725769407441426\n",
            "validation accuracy in epoch:  46  is  46.36326530612245\n",
            "training accuracy in epoch:  47  is  53.74000918695452\n",
            "validation accuracy in epoch:  47  is  51.82448979591836\n",
            "training accuracy in epoch:  48  is  54.547083141938444\n",
            "validation accuracy in epoch:  48  is  46.746938775510195\n",
            "training accuracy in epoch:  49  is  55.064308681672024\n",
            "validation accuracy in epoch:  49  is  52.69795918367347\n",
            "training accuracy in epoch:  50  is  55.73036288470372\n",
            "validation accuracy in epoch:  50  is  47.73469387755102\n",
            "training accuracy in epoch:  51  is  55.738171796049606\n",
            "validation accuracy in epoch:  51  is  48.57551020408163\n",
            "training accuracy in epoch:  52  is  55.859439595774\n",
            "validation accuracy in epoch:  52  is  53.8326530612245\n",
            "training accuracy in epoch:  53  is  56.48001837390906\n",
            "validation accuracy in epoch:  53  is  48.83265306122449\n",
            "training accuracy in epoch:  54  is  57.11575562700965\n",
            "validation accuracy in epoch:  54  is  50.730612244897955\n",
            "training accuracy in epoch:  55  is  57.538355535140106\n",
            "validation accuracy in epoch:  55  is  51.13877551020408\n",
            "training accuracy in epoch:  56  is  57.866329811667434\n",
            "validation accuracy in epoch:  56  is  49.86938775510203\n",
            "training accuracy in epoch:  57  is  57.736334405144696\n",
            "validation accuracy in epoch:  57  is  54.67755102040817\n",
            "training accuracy in epoch:  58  is  58.467615985300874\n",
            "validation accuracy in epoch:  58  is  51.68571428571428\n",
            "training accuracy in epoch:  59  is  58.967845659163984\n",
            "validation accuracy in epoch:  59  is  53.097959183673474\n",
            "training accuracy in epoch:  60  is  59.48507119889756\n",
            "validation accuracy in epoch:  60  is  54.43265306122449\n",
            "training accuracy in epoch:  61  is  59.48369315571888\n",
            "validation accuracy in epoch:  61  is  53.28163265306123\n",
            "training accuracy in epoch:  62  is  59.51814423518604\n",
            "validation accuracy in epoch:  62  is  59.44897959183674\n",
            "training accuracy in epoch:  63  is  60.239320165365186\n",
            "validation accuracy in epoch:  63  is  52.04489795918368\n",
            "training accuracy in epoch:  64  is  60.60496095544326\n",
            "validation accuracy in epoch:  64  is  57.76326530612245\n",
            "training accuracy in epoch:  65  is  61.092328892971985\n",
            "validation accuracy in epoch:  65  is  56.918367346938766\n",
            "training accuracy in epoch:  66  is  61.20119430408819\n",
            "validation accuracy in epoch:  66  is  56.208163265306126\n",
            "training accuracy in epoch:  67  is  61.100137804317875\n",
            "validation accuracy in epoch:  67  is  62.11836734693877\n",
            "training accuracy in epoch:  68  is  61.92191088654111\n",
            "validation accuracy in epoch:  68  is  54.60000000000001\n",
            "training accuracy in epoch:  69  is  62.43178686265504\n",
            "validation accuracy in epoch:  69  is  60.70612244897959\n",
            "training accuracy in epoch:  70  is  62.53330271015159\n",
            "validation accuracy in epoch:  70  is  56.74285714285715\n",
            "training accuracy in epoch:  71  is  62.73909049150206\n",
            "validation accuracy in epoch:  71  is  54.93469387755103\n",
            "training accuracy in epoch:  72  is  62.64997703261369\n",
            "validation accuracy in epoch:  72  is  63.53061224489795\n",
            "training accuracy in epoch:  73  is  63.13091410197519\n",
            "validation accuracy in epoch:  73  is  57.59591836734693\n",
            "training accuracy in epoch:  74  is  63.749196141479096\n",
            "validation accuracy in epoch:  74  is  60.39591836734693\n",
            "training accuracy in epoch:  75  is  64.1198897565457\n",
            "validation accuracy in epoch:  75  is  61.11836734693878\n",
            "training accuracy in epoch:  76  is  64.09600367478181\n",
            "validation accuracy in epoch:  76  is  62.14693877551019\n",
            "training accuracy in epoch:  77  is  64.1598530087276\n",
            "validation accuracy in epoch:  77  is  65.99591836734696\n",
            "training accuracy in epoch:  78  is  64.65640790078089\n",
            "validation accuracy in epoch:  78  is  60.34285714285714\n",
            "training accuracy in epoch:  79  is  65.14836931557188\n",
            "validation accuracy in epoch:  79  is  63.20408163265305\n",
            "training accuracy in epoch:  80  is  65.43132751492881\n",
            "validation accuracy in epoch:  80  is  64.45306122448979\n",
            "training accuracy in epoch:  81  is  65.40376665135507\n",
            "validation accuracy in epoch:  81  is  61.80408163265305\n",
            "training accuracy in epoch:  82  is  65.35369774919614\n",
            "validation accuracy in epoch:  82  is  67.0734693877551\n",
            "training accuracy in epoch:  83  is  65.89435002296739\n",
            "validation accuracy in epoch:  83  is  60.82448979591836\n",
            "training accuracy in epoch:  84  is  66.50068902158934\n",
            "validation accuracy in epoch:  84  is  66.00000000000001\n",
            "training accuracy in epoch:  85  is  66.64951768488746\n",
            "validation accuracy in epoch:  85  is  69.0612244897959\n",
            "training accuracy in epoch:  86  is  66.60817638952688\n",
            "validation accuracy in epoch:  86  is  59.00816326530612\n",
            "training accuracy in epoch:  87  is  66.59485530546624\n",
            "validation accuracy in epoch:  87  is  68.16734693877552\n",
            "training accuracy in epoch:  88  is  67.01515847496556\n",
            "validation accuracy in epoch:  88  is  65.82857142857142\n",
            "training accuracy in epoch:  89  is  67.29995406522738\n",
            "validation accuracy in epoch:  89  is  63.269387755102045\n",
            "training accuracy in epoch:  90  is  67.62103812586128\n",
            "validation accuracy in epoch:  90  is  68.87755102040815\n",
            "training accuracy in epoch:  91  is  67.79926504363803\n",
            "validation accuracy in epoch:  91  is  64.11836734693878\n",
            "training accuracy in epoch:  92  is  67.59531465319246\n",
            "validation accuracy in epoch:  92  is  69.91428571428573\n",
            "training accuracy in epoch:  93  is  68.22094625631603\n",
            "validation accuracy in epoch:  93  is  65.68163265306123\n",
            "training accuracy in epoch:  94  is  68.65411116214973\n",
            "validation accuracy in epoch:  94  is  68.53877551020409\n",
            "training accuracy in epoch:  95  is  68.74644005512172\n",
            "validation accuracy in epoch:  95  is  68.62448979591838\n",
            "training accuracy in epoch:  96  is  69.08038585209003\n",
            "validation accuracy in epoch:  96  is  64.39591836734695\n",
            "training accuracy in epoch:  97  is  68.71290767110703\n",
            "validation accuracy in epoch:  97  is  69.75510204081633\n",
            "training accuracy in epoch:  98  is  69.25677537896186\n",
            "validation accuracy in epoch:  98  is  68.39591836734694\n",
            "training accuracy in epoch:  99  is  69.39687643546165\n",
            "validation accuracy in epoch:  99  is  66.20408163265306\n",
            "training accuracy in epoch:  100  is  69.7781350482315\n",
            "validation accuracy in epoch:  100  is  72.35102040816327\n",
            "training accuracy in epoch:  101  is  69.90169958658704\n",
            "validation accuracy in epoch:  101  is  64.59591836734694\n",
            "training accuracy in epoch:  102  is  69.48231511254019\n",
            "validation accuracy in epoch:  102  is  70.73469387755104\n",
            "training accuracy in epoch:  103  is  70.21819016995866\n",
            "validation accuracy in epoch:  103  is  69.75510204081631\n",
            "training accuracy in epoch:  104  is  70.4170877354157\n",
            "validation accuracy in epoch:  104  is  68.98775510204081\n",
            "training accuracy in epoch:  105  is  70.63022508038586\n",
            "validation accuracy in epoch:  105  is  70.69795918367348\n",
            "training accuracy in epoch:  106  is  70.81258612769868\n",
            "validation accuracy in epoch:  106  is  66.41632653061225\n",
            "training accuracy in epoch:  107  is  70.56683509416627\n",
            "validation accuracy in epoch:  107  is  73.13469387755102\n",
            "training accuracy in epoch:  108  is  70.98851630684429\n",
            "validation accuracy in epoch:  108  is  70.48979591836735\n",
            "training accuracy in epoch:  109  is  71.27193385392742\n",
            "validation accuracy in epoch:  109  is  69.19183673469387\n",
            "training accuracy in epoch:  110  is  71.6118511713367\n",
            "validation accuracy in epoch:  110  is  72.69795918367346\n",
            "training accuracy in epoch:  111  is  71.44878272852549\n",
            "validation accuracy in epoch:  111  is  68.01632653061223\n",
            "training accuracy in epoch:  112  is  71.41433164905834\n",
            "validation accuracy in epoch:  112  is  72.58775510204082\n",
            "training accuracy in epoch:  113  is  71.77032613688561\n",
            "validation accuracy in epoch:  113  is  72.24489795918367\n",
            "training accuracy in epoch:  114  is  72.016077170418\n",
            "validation accuracy in epoch:  114  is  68.56326530612245\n",
            "training accuracy in epoch:  115  is  72.39182361047314\n",
            "validation accuracy in epoch:  115  is  77.19183673469388\n",
            "training accuracy in epoch:  116  is  72.57877813504822\n",
            "validation accuracy in epoch:  116  is  69.33877551020407\n",
            "training accuracy in epoch:  117  is  72.01561782269178\n",
            "validation accuracy in epoch:  117  is  74.18367346938776\n",
            "training accuracy in epoch:  118  is  72.48507119889756\n",
            "validation accuracy in epoch:  118  is  71.15102040816326\n",
            "training accuracy in epoch:  119  is  72.89894350022968\n",
            "validation accuracy in epoch:  119  is  71.87346938775508\n",
            "training accuracy in epoch:  120  is  73.02158934313276\n",
            "validation accuracy in epoch:  120  is  77.8204081632653\n",
            "training accuracy in epoch:  121  is  73.09416628387689\n",
            "validation accuracy in epoch:  121  is  72.35918367346939\n",
            "training accuracy in epoch:  122  is  72.93431327514928\n",
            "validation accuracy in epoch:  122  is  75.1591836734694\n",
            "training accuracy in epoch:  123  is  73.19751952227837\n",
            "validation accuracy in epoch:  123  is  75.91428571428573\n",
            "training accuracy in epoch:  124  is  73.47864033073037\n",
            "validation accuracy in epoch:  124  is  71.3591836734694\n",
            "training accuracy in epoch:  125  is  73.7473587505742\n",
            "validation accuracy in epoch:  125  is  77.86938775510204\n",
            "training accuracy in epoch:  126  is  73.56407900780891\n",
            "validation accuracy in epoch:  126  is  71.95918367346938\n",
            "training accuracy in epoch:  127  is  73.50987597611392\n",
            "validation accuracy in epoch:  127  is  77.42448979591836\n",
            "training accuracy in epoch:  128  is  73.90399632521819\n",
            "validation accuracy in epoch:  128  is  78.28163265306124\n",
            "training accuracy in epoch:  129  is  74.15755627009646\n",
            "validation accuracy in epoch:  129  is  70.82448979591838\n",
            "training accuracy in epoch:  130  is  74.28203950390444\n",
            "validation accuracy in epoch:  130  is  77.71836734693876\n",
            "training accuracy in epoch:  131  is  74.2397795130914\n",
            "validation accuracy in epoch:  131  is  72.01224489795918\n",
            "training accuracy in epoch:  132  is  74.04915020670649\n",
            "validation accuracy in epoch:  132  is  75.66938775510205\n",
            "training accuracy in epoch:  133  is  74.55810748736793\n",
            "validation accuracy in epoch:  133  is  76.1061224489796\n",
            "training accuracy in epoch:  134  is  74.74184657785945\n",
            "validation accuracy in epoch:  134  is  71.5469387755102\n",
            "training accuracy in epoch:  135  is  74.8690858980248\n",
            "validation accuracy in epoch:  135  is  81.59999999999998\n",
            "training accuracy in epoch:  136  is  74.98897565457051\n",
            "validation accuracy in epoch:  136  is  72.66530612244898\n",
            "training accuracy in epoch:  137  is  74.75378961874138\n",
            "validation accuracy in epoch:  137  is  77.57142857142857\n",
            "training accuracy in epoch:  138  is  75.25769407441433\n",
            "validation accuracy in epoch:  138  is  78.04489795918367\n",
            "training accuracy in epoch:  139  is  75.2475884244373\n",
            "validation accuracy in epoch:  139  is  74.68979591836734\n",
            "training accuracy in epoch:  140  is  75.44648598989436\n",
            "validation accuracy in epoch:  140  is  75.46122448979594\n",
            "training accuracy in epoch:  141  is  75.61368856224162\n",
            "validation accuracy in epoch:  141  is  75.12244897959184\n",
            "training accuracy in epoch:  142  is  75.14607257694075\n",
            "validation accuracy in epoch:  142  is  77.72653061224491\n",
            "training accuracy in epoch:  143  is  75.80064308681672\n",
            "validation accuracy in epoch:  143  is  78.7061224489796\n",
            "training accuracy in epoch:  144  is  75.99954065227378\n",
            "validation accuracy in epoch:  144  is  76.79183673469386\n",
            "training accuracy in epoch:  145  is  76.07487367937529\n",
            "validation accuracy in epoch:  145  is  80.04081632653062\n",
            "training accuracy in epoch:  146  is  76.30546623794211\n",
            "validation accuracy in epoch:  146  is  74.75102040816327\n",
            "training accuracy in epoch:  147  is  75.77124483233807\n",
            "validation accuracy in epoch:  147  is  78.80816326530613\n",
            "training accuracy in epoch:  148  is  76.2365640790078\n",
            "validation accuracy in epoch:  148  is  80.08979591836734\n",
            "training accuracy in epoch:  149  is  76.56821313734496\n",
            "validation accuracy in epoch:  149  is  74.46938775510203\n",
            "training accuracy in epoch:  150  is  76.53100597152043\n",
            "validation accuracy in epoch:  150  is  79.14285714285714\n",
            "training accuracy in epoch:  151  is  76.61276986678915\n",
            "validation accuracy in epoch:  151  is  78.60408163265305\n",
            "training accuracy in epoch:  152  is  76.28203950390446\n",
            "validation accuracy in epoch:  152  is  80.2734693877551\n",
            "training accuracy in epoch:  153  is  76.65916398713826\n",
            "validation accuracy in epoch:  153  is  79.44897959183673\n",
            "training accuracy in epoch:  154  is  76.99954065227378\n",
            "validation accuracy in epoch:  154  is  76.7469387755102\n",
            "training accuracy in epoch:  155  is  76.96922370234267\n",
            "validation accuracy in epoch:  155  is  81.33877551020407\n",
            "training accuracy in epoch:  156  is  77.14974735875057\n",
            "validation accuracy in epoch:  156  is  78.5265306122449\n",
            "training accuracy in epoch:  157  is  76.7156637574644\n",
            "validation accuracy in epoch:  157  is  80.59999999999998\n",
            "training accuracy in epoch:  158  is  76.99127239320167\n",
            "validation accuracy in epoch:  158  is  80.6816326530612\n",
            "training accuracy in epoch:  159  is  77.49242076251721\n",
            "validation accuracy in epoch:  159  is  75.63673469387754\n",
            "training accuracy in epoch:  160  is  77.4712907671107\n",
            "validation accuracy in epoch:  160  is  83.01632653061226\n",
            "training accuracy in epoch:  161  is  77.42076251722554\n",
            "validation accuracy in epoch:  161  is  77.14285714285714\n",
            "training accuracy in epoch:  162  is  77.10289389067525\n",
            "validation accuracy in epoch:  162  is  81.87755102040815\n",
            "training accuracy in epoch:  163  is  77.52595314653192\n",
            "validation accuracy in epoch:  163  is  82.18367346938774\n",
            "training accuracy in epoch:  164  is  77.77262287551676\n",
            "validation accuracy in epoch:  164  is  75.61224489795917\n",
            "training accuracy in epoch:  165  is  77.85254937988057\n",
            "validation accuracy in epoch:  165  is  84.69387755102044\n",
            "training accuracy in epoch:  166  is  77.83876894809372\n",
            "validation accuracy in epoch:  166  is  79.31020408163263\n",
            "training accuracy in epoch:  167  is  77.3242994947175\n",
            "validation accuracy in epoch:  167  is  81.20408163265306\n",
            "training accuracy in epoch:  168  is  77.80753330271014\n",
            "validation accuracy in epoch:  168  is  80.05714285714286\n",
            "training accuracy in epoch:  169  is  78.29214515388148\n",
            "validation accuracy in epoch:  169  is  75.48571428571428\n",
            "training accuracy in epoch:  170  is  78.15388148828664\n",
            "validation accuracy in epoch:  170  is  84.26122448979592\n",
            "training accuracy in epoch:  171  is  78.19751952227837\n",
            "validation accuracy in epoch:  171  is  80.84081632653063\n",
            "training accuracy in epoch:  172  is  77.9200734956362\n",
            "validation accuracy in epoch:  172  is  81.69795918367349\n",
            "training accuracy in epoch:  173  is  78.26136885622417\n",
            "validation accuracy in epoch:  173  is  82.21224489795918\n",
            "training accuracy in epoch:  174  is  78.49747358750574\n",
            "validation accuracy in epoch:  174  is  77.38775510204081\n",
            "training accuracy in epoch:  175  is  78.56545705098759\n",
            "validation accuracy in epoch:  175  is  81.30612244897958\n",
            "training accuracy in epoch:  176  is  78.55902618282039\n",
            "validation accuracy in epoch:  176  is  81.62857142857143\n",
            "training accuracy in epoch:  177  is  78.19476343592099\n",
            "validation accuracy in epoch:  177  is  81.79999999999998\n",
            "training accuracy in epoch:  178  is  78.7060174552136\n",
            "validation accuracy in epoch:  178  is  81.6530612244898\n",
            "training accuracy in epoch:  179  is  78.90261828203951\n",
            "validation accuracy in epoch:  179  is  77.14693877551021\n",
            "training accuracy in epoch:  180  is  79.0463941203491\n",
            "validation accuracy in epoch:  180  is  86.1061224489796\n",
            "training accuracy in epoch:  181  is  78.94993109784107\n",
            "validation accuracy in epoch:  181  is  81.14285714285714\n",
            "training accuracy in epoch:  182  is  78.6357372531006\n",
            "validation accuracy in epoch:  182  is  83.71836734693876\n",
            "training accuracy in epoch:  183  is  78.99862195682131\n",
            "validation accuracy in epoch:  183  is  82.86122448979593\n",
            "training accuracy in epoch:  184  is  79.23197060174552\n",
            "validation accuracy in epoch:  184  is  77.8\n",
            "training accuracy in epoch:  185  is  79.16628387689481\n",
            "validation accuracy in epoch:  185  is  85.22448979591836\n",
            "training accuracy in epoch:  186  is  79.29627928341755\n",
            "validation accuracy in epoch:  186  is  82.15510204081633\n",
            "training accuracy in epoch:  187  is  78.91042719338539\n",
            "validation accuracy in epoch:  187  is  84.46122448979592\n",
            "training accuracy in epoch:  188  is  79.46899402847956\n",
            "validation accuracy in epoch:  188  is  83.6\n",
            "training accuracy in epoch:  189  is  79.56178226917778\n",
            "validation accuracy in epoch:  189  is  77.33061224489795\n",
            "training accuracy in epoch:  190  is  79.67248507119889\n",
            "validation accuracy in epoch:  190  is  84.52244897959183\n",
            "training accuracy in epoch:  191  is  79.70188332567754\n",
            "validation accuracy in epoch:  191  is  82.38367346938776\n",
            "training accuracy in epoch:  192  is  79.22921451538814\n",
            "validation accuracy in epoch:  192  is  84.15510204081633\n",
            "training accuracy in epoch:  193  is  79.49104271933854\n",
            "validation accuracy in epoch:  193  is  83.89795918367348\n",
            "training accuracy in epoch:  194  is  79.98438217730822\n",
            "validation accuracy in epoch:  194  is  77.84489795918367\n",
            "training accuracy in epoch:  195  is  79.7859439595774\n",
            "validation accuracy in epoch:  195  is  83.49795918367347\n",
            "training accuracy in epoch:  196  is  79.76711070280203\n",
            "validation accuracy in epoch:  196  is  81.21632653061224\n",
            "training accuracy in epoch:  197  is  79.64079007808911\n",
            "validation accuracy in epoch:  197  is  84.10612244897959\n",
            "training accuracy in epoch:  198  is  79.97197978870005\n",
            "validation accuracy in epoch:  198  is  82.28979591836735\n",
            "training accuracy in epoch:  199  is  80.07992650436381\n",
            "validation accuracy in epoch:  199  is  77.82857142857142\n",
            "training accuracy in epoch:  200  is  80.0463941203491\n",
            "validation accuracy in epoch:  200  is  84.82857142857141\n",
            "training accuracy in epoch:  201  is  79.86449242076252\n",
            "validation accuracy in epoch:  201  is  82.17959183673469\n",
            "training accuracy in epoch:  202  is  79.82590721175931\n",
            "validation accuracy in epoch:  202  is  85.1265306122449\n",
            "training accuracy in epoch:  203  is  80.14101975195223\n",
            "validation accuracy in epoch:  203  is  83.22857142857143\n",
            "training accuracy in epoch:  204  is  80.47634359209921\n",
            "validation accuracy in epoch:  204  is  78.64897959183672\n",
            "training accuracy in epoch:  205  is  80.35553514010105\n",
            "validation accuracy in epoch:  205  is  84.45306122448977\n",
            "training accuracy in epoch:  206  is  80.32705558107486\n",
            "validation accuracy in epoch:  206  is  82.67755102040816\n",
            "training accuracy in epoch:  207  is  80.1644464859899\n",
            "validation accuracy in epoch:  207  is  85.67755102040816\n",
            "training accuracy in epoch:  208  is  80.3178686265503\n",
            "validation accuracy in epoch:  208  is  81.90612244897959\n",
            "training accuracy in epoch:  209  is  80.81855764813965\n",
            "validation accuracy in epoch:  209  is  81.42040816326531\n",
            "training accuracy in epoch:  210  is  80.66192007349564\n",
            "validation accuracy in epoch:  210  is  83.93469387755101\n",
            "training accuracy in epoch:  211  is  80.57096922370233\n",
            "validation accuracy in epoch:  211  is  84.44081632653061\n",
            "training accuracy in epoch:  212  is  80.27882406982086\n",
            "validation accuracy in epoch:  212  is  85.56734693877549\n",
            "training accuracy in epoch:  213  is  80.50711988975655\n",
            "validation accuracy in epoch:  213  is  84.09795918367347\n",
            "training accuracy in epoch:  214  is  80.91731740927882\n",
            "validation accuracy in epoch:  214  is  79.24081632653062\n",
            "training accuracy in epoch:  215  is  80.79513091410199\n",
            "validation accuracy in epoch:  215  is  84.91428571428573\n",
            "training accuracy in epoch:  216  is  80.9219108865411\n",
            "validation accuracy in epoch:  216  is  82.89387755102042\n",
            "training accuracy in epoch:  217  is  80.64813964170877\n",
            "validation accuracy in epoch:  217  is  86.34693877551021\n",
            "training accuracy in epoch:  218  is  80.92099219108866\n",
            "validation accuracy in epoch:  218  is  83.94285714285715\n",
            "training accuracy in epoch:  219  is  81.2558566835094\n",
            "validation accuracy in epoch:  219  is  79.19591836734695\n",
            "training accuracy in epoch:  220  is  81.14101975195223\n",
            "validation accuracy in epoch:  220  is  85.06530612244899\n",
            "training accuracy in epoch:  221  is  81.070280202113\n",
            "validation accuracy in epoch:  221  is  82.92244897959183\n",
            "training accuracy in epoch:  222  is  80.68764354616445\n",
            "validation accuracy in epoch:  222  is  85.27755102040815\n",
            "training accuracy in epoch:  223  is  81.09232889297196\n",
            "validation accuracy in epoch:  223  is  84.36734693877551\n",
            "training accuracy in epoch:  224  is  81.39457969683049\n",
            "validation accuracy in epoch:  224  is  79.0326530612245\n",
            "training accuracy in epoch:  225  is  81.2333486449242\n",
            "validation accuracy in epoch:  225  is  85.41632653061224\n",
            "training accuracy in epoch:  226  is  81.19889756545706\n",
            "validation accuracy in epoch:  226  is  83.91428571428571\n",
            "training accuracy in epoch:  227  is  80.83876894809372\n",
            "validation accuracy in epoch:  227  is  86.46122448979594\n",
            "training accuracy in epoch:  228  is  81.3739090491502\n",
            "validation accuracy in epoch:  228  is  83.7795918367347\n",
            "training accuracy in epoch:  229  is  81.47726228755168\n",
            "validation accuracy in epoch:  229  is  80.32244897959184\n",
            "training accuracy in epoch:  230  is  81.54386770785484\n",
            "validation accuracy in epoch:  230  is  85.68571428571427\n",
            "training accuracy in epoch:  231  is  81.3964170877354\n",
            "validation accuracy in epoch:  231  is  83.41224489795917\n",
            "training accuracy in epoch:  232  is  81.1373449701424\n",
            "validation accuracy in epoch:  232  is  85.24489795918367\n",
            "training accuracy in epoch:  233  is  81.21221864951768\n",
            "validation accuracy in epoch:  233  is  81.83265306122449\n",
            "training accuracy in epoch:  234  is  81.84979329352319\n",
            "validation accuracy in epoch:  234  is  81.68571428571427\n",
            "training accuracy in epoch:  235  is  81.72255397335783\n",
            "validation accuracy in epoch:  235  is  87.40408163265306\n",
            "training accuracy in epoch:  236  is  81.65411116214975\n",
            "validation accuracy in epoch:  236  is  83.44489795918366\n",
            "training accuracy in epoch:  237  is  81.2508038585209\n",
            "validation accuracy in epoch:  237  is  85.48979591836734\n",
            "training accuracy in epoch:  238  is  81.70463941203492\n",
            "validation accuracy in epoch:  238  is  82.53877551020409\n",
            "training accuracy in epoch:  239  is  82.11621497473587\n",
            "validation accuracy in epoch:  239  is  81.1265306122449\n",
            "training accuracy in epoch:  240  is  81.96784565916398\n",
            "validation accuracy in epoch:  240  is  88.31020408163263\n",
            "training accuracy in epoch:  241  is  81.78640330730364\n",
            "validation accuracy in epoch:  241  is  83.21632653061224\n",
            "training accuracy in epoch:  242  is  81.66789159393662\n",
            "validation accuracy in epoch:  242  is  86.49795918367347\n",
            "training accuracy in epoch:  243  is  81.86449242076253\n",
            "validation accuracy in epoch:  243  is  84.64081632653061\n",
            "training accuracy in epoch:  244  is  82.30592558566835\n",
            "validation accuracy in epoch:  244  is  82.65714285714284\n",
            "training accuracy in epoch:  245  is  82.09049150206707\n",
            "validation accuracy in epoch:  245  is  88.09387755102041\n",
            "training accuracy in epoch:  246  is  82.09278824069821\n",
            "validation accuracy in epoch:  246  is  83.03673469387756\n",
            "training accuracy in epoch:  247  is  81.7491961414791\n",
            "validation accuracy in epoch:  247  is  87.28163265306124\n",
            "training accuracy in epoch:  248  is  82.00091869545247\n",
            "validation accuracy in epoch:  248  is  84.33061224489795\n",
            "training accuracy in epoch:  249  is  82.53146531924666\n",
            "validation accuracy in epoch:  249  is  81.23673469387755\n",
            "training accuracy in epoch:  250  is  82.29811667432246\n",
            "validation accuracy in epoch:  250  is  88.13877551020407\n",
            "training accuracy in epoch:  251  is  82.11254019292603\n",
            "validation accuracy in epoch:  251  is  83.99591836734693\n",
            "training accuracy in epoch:  252  is  81.94441892512633\n",
            "validation accuracy in epoch:  252  is  86.73061224489796\n",
            "training accuracy in epoch:  253  is  82.0032154340836\n",
            "validation accuracy in epoch:  253  is  85.71020408163264\n",
            "training accuracy in epoch:  254  is  82.60174552135966\n",
            "validation accuracy in epoch:  254  is  81.48979591836735\n",
            "training accuracy in epoch:  255  is  82.38860817638954\n",
            "validation accuracy in epoch:  255  is  87.91020408163263\n",
            "training accuracy in epoch:  256  is  82.32705558107486\n",
            "validation accuracy in epoch:  256  is  84.32244897959183\n",
            "training accuracy in epoch:  257  is  81.9683050068902\n",
            "validation accuracy in epoch:  257  is  82.98775510204081\n",
            "training accuracy in epoch:  258  is  82.32292145153882\n",
            "validation accuracy in epoch:  258  is  85.98367346938775\n",
            "training accuracy in epoch:  259  is  82.53146531924666\n",
            "validation accuracy in epoch:  259  is  82.22040816326529\n",
            "training accuracy in epoch:  260  is  82.60542030316951\n",
            "validation accuracy in epoch:  260  is  86.77142857142859\n",
            "training accuracy in epoch:  261  is  82.39549839228296\n",
            "validation accuracy in epoch:  261  is  85.09387755102041\n",
            "training accuracy in epoch:  262  is  82.39320165365181\n",
            "validation accuracy in epoch:  262  is  87.29795918367346\n",
            "training accuracy in epoch:  263  is  82.46991272393201\n",
            "validation accuracy in epoch:  263  is  85.46938775510206\n",
            "training accuracy in epoch:  264  is  82.77537896187414\n",
            "validation accuracy in epoch:  264  is  80.57959183673469\n",
            "training accuracy in epoch:  265  is  82.71520440973818\n",
            "validation accuracy in epoch:  265  is  88.11836734693878\n",
            "training accuracy in epoch:  266  is  82.60128617363344\n",
            "validation accuracy in epoch:  266  is  84.44081632653061\n",
            "training accuracy in epoch:  267  is  82.23105190629308\n",
            "validation accuracy in epoch:  267  is  83.86938775510204\n",
            "training accuracy in epoch:  268  is  82.62241616903997\n",
            "validation accuracy in epoch:  268  is  85.82448979591837\n",
            "training accuracy in epoch:  269  is  82.80477721635278\n",
            "validation accuracy in epoch:  269  is  81.15918367346941\n",
            "training accuracy in epoch:  270  is  82.74460266421681\n",
            "validation accuracy in epoch:  270  is  85.7795918367347\n",
            "training accuracy in epoch:  271  is  82.70096463022509\n",
            "validation accuracy in epoch:  271  is  84.80816326530612\n",
            "training accuracy in epoch:  272  is  82.54294901240239\n",
            "validation accuracy in epoch:  272  is  87.75102040816327\n",
            "training accuracy in epoch:  273  is  82.63987138263664\n",
            "validation accuracy in epoch:  273  is  85.5877551020408\n",
            "training accuracy in epoch:  274  is  82.98621956821314\n",
            "validation accuracy in epoch:  274  is  80.93061224489796\n",
            "training accuracy in epoch:  275  is  82.88745980707395\n",
            "validation accuracy in epoch:  275  is  86.77142857142857\n",
            "training accuracy in epoch:  276  is  82.91961414790997\n",
            "validation accuracy in epoch:  276  is  84.76326530612246\n",
            "training accuracy in epoch:  277  is  82.63114377583832\n",
            "validation accuracy in epoch:  277  is  86.35102040816327\n",
            "training accuracy in epoch:  278  is  82.8736793752871\n",
            "validation accuracy in epoch:  278  is  88.1877551020408\n",
            "training accuracy in epoch:  279  is  83.36242535599447\n",
            "validation accuracy in epoch:  279  is  83.2408163265306\n",
            "training accuracy in epoch:  280  is  83.09370693615065\n",
            "validation accuracy in epoch:  280  is  89.22857142857143\n",
            "training accuracy in epoch:  281  is  83.11759301791456\n",
            "validation accuracy in epoch:  281  is  85.1142857142857\n",
            "training accuracy in epoch:  282  is  82.75103353238401\n",
            "validation accuracy in epoch:  282  is  87.05714285714286\n",
            "training accuracy in epoch:  283  is  82.98254478640331\n",
            "validation accuracy in epoch:  283  is  85.87755102040816\n",
            "training accuracy in epoch:  284  is  83.36150666054203\n",
            "validation accuracy in epoch:  284  is  82.02448979591837\n",
            "training accuracy in epoch:  285  is  83.21175930179145\n",
            "validation accuracy in epoch:  285  is  89.64081632653063\n",
            "training accuracy in epoch:  286  is  82.96417087735416\n",
            "validation accuracy in epoch:  286  is  83.98775510204082\n",
            "training accuracy in epoch:  287  is  82.91731740927882\n",
            "validation accuracy in epoch:  287  is  82.80000000000001\n",
            "training accuracy in epoch:  288  is  83.21221864951768\n",
            "validation accuracy in epoch:  288  is  86.3265306122449\n",
            "training accuracy in epoch:  289  is  83.44189251263207\n",
            "validation accuracy in epoch:  289  is  82.10612244897958\n",
            "training accuracy in epoch:  290  is  83.39503904455673\n",
            "validation accuracy in epoch:  290  is  89.24081632653063\n",
            "training accuracy in epoch:  291  is  83.26136885622417\n",
            "validation accuracy in epoch:  291  is  83.99183673469386\n",
            "training accuracy in epoch:  292  is  83.06430868167203\n",
            "validation accuracy in epoch:  292  is  86.71428571428572\n",
            "training accuracy in epoch:  293  is  83.19614147909968\n",
            "validation accuracy in epoch:  293  is  86.42448979591836\n",
            "training accuracy in epoch:  294  is  83.59209921910886\n",
            "validation accuracy in epoch:  294  is  82.28571428571429\n",
            "training accuracy in epoch:  295  is  83.43270555810749\n",
            "validation accuracy in epoch:  295  is  87.57959183673469\n",
            "training accuracy in epoch:  296  is  83.2746899402848\n",
            "validation accuracy in epoch:  296  is  84.95918367346938\n",
            "training accuracy in epoch:  297  is  83.19568213137346\n",
            "validation accuracy in epoch:  297  is  86.90612244897959\n",
            "training accuracy in epoch:  298  is  83.45107946715665\n",
            "validation accuracy in epoch:  298  is  87.28979591836735\n",
            "training accuracy in epoch:  299  is  83.63206247129077\n",
            "validation accuracy in epoch:  299  is  83.05714285714285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L856xl4UPKUO",
        "colab_type": "code",
        "outputId": "9843fee4-b8cf-4648-ca94-3ab83391bad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "plt.plot([i+1 for i in range(num_epochs)],mean_valid_acc,label=\"validation accuracy\")\n",
        "plt.plot([i+1 for i in range(num_epochs)],mean_train_acc,label=\"train accuracy\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"Epohs\")\n",
        "plt.ylabel(\"accuracy in % \")\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3XhklAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nkLwwJWMfTp",
        "colab_type": "code",
        "outputId": "9528dd5d-595e-438f-c182-407da8c96087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#generating words\n",
        "print(\"GENERATING FAKE SHAKESPEARE TEXT: \\n________________________________________________________\")\n",
        "num_words=2000 # th number of words that we want to generate\n",
        "testacc=[]\n",
        "\n",
        "## what is the first word?  answer:  forx example The\n",
        "input_word_index=return_index('The')\n",
        "print('The',end=\" \")\n",
        "X=torch.autograd.Variable(torch.LongTensor(np.array([input_word_index]).reshape(1,1)))\n",
        "hidden = model.init_hidden(1)\n",
        "\n",
        "for i in range(num_words):\n",
        "\n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "           \n",
        "  \n",
        "  \n",
        "#     print(hidden.shape)\n",
        "    outputs,hidden=model(X,hidden)# outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(vocab_size)\n",
        "    outExp=outputs.exp()\n",
        "#     print(outputs)\n",
        "    probablistic_output=(outExp)/(outExp.sum())# probablistic_output shape: torch.Size([vocab_size])\n",
        "#     probablistic_output=((outputs)/(0.4)).exp()\n",
        "\n",
        "    word_index = (torch.multinomial(probablistic_output, 1))\n",
        "    X=torch.autograd.Variable((word_index).reshape(1,1))\n",
        "    if(voc[word_index]==\"<end>\"):\n",
        "      print(\"\")\n",
        "    else:\n",
        "      print(voc[word_index],end=\" \")\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GENERATING FAKE SHAKESPEARE TEXT: \n",
            "________________________________________________________\n",
            "The pine and let thee out. \n",
            "\n",
            "KING RICHARD III: \n",
            "And long let me be pleased, what's that to seek again. \n",
            "\n",
            "KING RICHARD II: \n",
            "Norfolk, throw down, we bid; there is no boot. \n",
            "\n",
            "THOMAS MOWBRAY: \n",
            "Myself I throw, dread sovereign, at thy foot. \n",
            "My life thou shalt command, but not my shame: \n",
            "The one that calls our sentence then, the fault's thus manifested; \n",
            "Which, though thou wouldst for well afford a grave \n",
            "As thou canst yield a melancholy seat! \n",
            "Then would I hide my bones, not rest them here. \n",
            "I do repent. Alas! I have long in this deep despair? \n",
            "\n",
            "QUEEN ELIZABETH: \n",
            "From one that long detain'd you from your grace. \n",
            "\n",
            "ROMEO: \n",
            "I hear no more: die, 'tis not a maid. \n",
            "\n",
            "DUCHESS: \n",
            "Where shall arm me resign my father, \n",
            "But what he can abet him in this kind \n",
            "Cherish rebellion and are rebels all. \n",
            "\n",
            "NORTHUMBERLAND: \n",
            "My right is noble and fair earth, \n",
            "Who lately made here? Yet thou had there I have spoke, \n",
            "My old man's Hence from my Lord of Buckingham? \n",
            "\n",
            "DUKE OF YORK: \n",
            "Nay, rather, every tedious stride I make \n",
            "Will but remember me what will there be sure of care? \n",
            "Strives Bolingbroke to be compell'd sins \n",
            "Stand such a show! \n",
            "\n",
            "KING RICHARD II: \n",
            "We are amazed; and thus long long have seen. \n",
            "What, came I horse! \n",
            "\n",
            "KING RICHARD II: \n",
            "Then will you go not another horse: bind up my wounds. \n",
            "Have mercy, Jesu!--Soft! I did but dream. \n",
            "O coward conscience, how dost thou afflict me! \n",
            "The lights burn blue. It is now dead midnight. \n",
            "Cold fearful drops stand on my trembling flesh. \n",
            "What do I fear? I fear, for some man would have you do. \n",
            "I can prevent the ways to wail. \n",
            "To fear the foe, since fear oppresseth strength, \n",
            "Gives in your weakness strength unto your foe, \n",
            "And so your follies fight against yourself. \n",
            "Fear and be slain; but what these sorrows could not wake. \n",
            "\n",
            "HENRY BOLINGBROKE: \n",
            "These differences shall all rest under gage \n",
            "Till Norfolk be repeal'd: repeal'd he are miscarried at sea? \n",
            "\n",
            "KING RICHARD II: \n",
            "I am too studying at the instant came \n",
            "The fiery Tybalt, with this scepter'd isle, \n",
            "This earth of majesty, and seat \n",
            "Shall sit the pack of fasting: they throng who \n",
            "In conscience and my late? \n",
            "\n",
            "ANGELO: \n",
            "Old John of Gaunt, time-honour'd Lancaster, \n",
            "Hast thou, according to thy oath and band, \n",
            "Brought hither Henry Hereford no league \n",
            "But milk my ewes and weep. \n",
            "\n",
            "HENRY BOLINGBROKE: \n",
            "What shrill-voiced suppliant makes this eager cry? \n",
            "\n",
            "DUCHESS OF YORK: \n",
            "A woman, and thy aunt, great king; 'tis I. \n",
            "Speak with me, pity me, open door. \n",
            "\n",
            "CAPULET: \n",
            "I say, that had. \n",
            "\n",
            "GREGORY: \n",
            "The exchange of thy love's faithful vow for mine. \n",
            "\n",
            "JULIET: \n",
            "Love give me strength! and strength shall help afford. \n",
            "Farewell, dear father! \n",
            "\n",
            "LADY CAPULET: \n",
            "Speak briefly, can you like of Paris' heart, \n",
            "And self-affrighted tremble at his sin. \n",
            "Not all the drooping thoughts which live \n",
            "Of my dear queen, and would she stir the peace. \n",
            "\n",
            "Nurse: \n",
            "O pretty oppression against a tender thing. \n",
            "\n",
            "ROMEO: \n",
            "Is love a tender thing? it is too rough, \n",
            "Too rude, too boisterous, and it pricks like thorn. \n",
            "\n",
            "MERCUTIO: \n",
            "If love be rough with you, be rough with love; \n",
            "Prick love for pricking, and you beat love down. \n",
            "Give me a case to put my visage in: \n",
            "A visor for a visor! what care I \n",
            "What curious eye doth quote deformities? \n",
            "Here are the beetle brows shall blush for me. \n",
            "\n",
            "BENVOLIO: \n",
            "Come, knock and enter; and no sooner in, \n",
            "But every man betake him to his legs. \n",
            "\n",
            "ROMEO: \n",
            "A torch for me: let wantons light of heart \n",
            "Tickle the senseless rushes with their heels, \n",
            "For I am proverb'd with a grandsire phrase; \n",
            "I'll be a candle-holder, and look on. \n",
            "The game was ne'er so fair, and I am done. \n",
            "\n",
            "MERCUTIO: \n",
            "Tut, dun's the mouse, the constable's own word: \n",
            "There is no matter like a man should be him. \n",
            "\n",
            "MERCUTIO: \n",
            "Why, cannot tell her, sort me word so fair? \n",
            "\n",
            "KING EDWARD IV: \n",
            "Hold, then; go home, be merry, give consent \n",
            "To marry Paris: Wednesday is to-morrow: \n",
            "To-morrow night look that thou lie alone; \n",
            "Let thy word lie in him \n",
            "To send him here to dim if if thou respect, \n",
            "Show a fair presence and put up his head. \n",
            "\n",
            "MONTAGUE: \n",
            "Good brother, as I am a subject mean to see my counsel, is but one \n",
            "To Lammas-tide? \n",
            "\n",
            "LADY CAPULET: \n",
            "A fortnight and odd days. \n",
            "\n",
            "Nurse: \n",
            "Even or odd, of all days in thy cheeks, \n",
            "And death's pale flag is not advanced there. \n",
            "Tybalt, liest thou there in thy bloody sheet? \n",
            "O, what more favour can do this the rest should privy to your daughter. \n",
            "If she have kill'd me here? I fear, alone, O sirrah! his noble kinsman! Prince, as thou art true, \n",
            "For all my number of a smock. \n",
            "\n",
            "Nurse: \n",
            "Peter! \n",
            "\n",
            "PETER: \n",
            "Anon! \n",
            "\n",
            "Nurse: \n",
            "My fan, Peter. \n",
            "\n",
            "MERCUTIO: \n",
            "Good Peter, to hide her face; for her fan's the \n",
            "fairer face. \n",
            "\n",
            "Nurse: \n",
            "God ye good morrow, gentlemen. \n",
            "\n",
            "MERCUTIO: \n",
            "God ye good den, fair gentlewoman. \n",
            "\n",
            "Nurse: \n",
            "Is it good den? \n",
            "\n",
            "MERCUTIO: \n",
            "'Tis no less, I tell you, for the bawdy hand of the \n",
            "dial is now upon the prick of noon. \n",
            "\n",
            "Nurse: \n",
            "Out upon you! what a manner that I dream not so? \n",
            "\n",
            "MERCUTIO: \n",
            "Yea, word at least, for ever I have slept; \n",
            "And we, in pity of the gentle king, \n",
            "He is not dead; \n",
            "And Paris shall revenge by earth, and leave of your bed; \n",
            "Throw on her body to the whole five: \n",
            "was I with you here for the goose? \n",
            "\n",
            "ROMEO: \n",
            "Thou wast thyself in the first and yet he were, \n",
            "And living here the king be spite must be to-morrow. \n",
            "I am too young for me. \n",
            "\n",
            "FRIAR LAURENCE: \n",
            "Romeo must pardon her. \n",
            "\n",
            "JULIET: \n",
            "Farewell, old York was gone, \n",
            "And hire those horses; I'll be with thee straight. \n",
            "Well, Juliet, I will lie with thee to-night. \n",
            "\n",
            "MERCUTIO: \n",
            "And this same needy man must sell it me. \n",
            "As I remember, this should be the king. \n",
            "\n",
            "KING HENRY VI: \n",
            "Ay, and so far from fair contrary as thou badest me, \n",
            "In troops I have in their defence: \n",
            "May that ground gape and swallow me alive, \n",
            "Where I shall kneel to him that slew my father! \n",
            "\n",
            "KING HENRY VI: \n",
            "O Clifford, how the loving mother? \n",
            "It is it right, and I am blows with me from \n",
            "Let me dispute with thee more, for thy kind prince, \n",
            "Is heap'd like mine and that thy skill be more \n",
            "To blazon it, then sweeten with thy breath \n",
            "This neighbour air, and let rich music's tongue \n",
            "Unfold the imagined happiness that both \n",
            "Receive in either by this dear encounter. \n",
            "\n",
            "JULIET: \n",
            "Conceit, more rich in matter than in joy! \n",
            "News from Verona!--How now, Balthasar! \n",
            "Dost thou not bring me letters from the friar? \n",
            "How doth my lady? Is my father well? \n",
            "How fares my Juliet? that I ask again; \n",
            "For nothing can not help him well. \n",
            "Sweet, sweet, sweet nurse, tell me, what says my friend. \n",
            "\n",
            "LADY CAPULET: \n",
            "What, shall you bind me to your highness' service. \n",
            "\n",
            "KING EDWARD IV: \n",
            "\n",
            "\n",
            "GLOUCESTER: \n",
            "Ay, for the reach of these my hands: \n",
            "Would none but I will venge my cousin's death! \n",
            "\n",
            "LADY GREY: \n",
            "To tell you in this bloody flag against all \n",
            "the prince my stay with you resign his crown \n",
            "But what thou fail'st--as God forbid the hour!-- \n",
            "Must Edward fall, which peril heaven forfend! \n",
            "\n",
            "WARWICK: \n",
            "No longer Earl of March, but Duke of York: \n",
            "The next degree is England's royal throne; \n",
            "For King of England shalt thou be proclaim'd \n",
            "In every borough as we pass along; \n",
            "And he that throws not up his cap for joy \n",
            "Shall for the fault make forfeit of his head. \n",
            "King Edward, valiant Richard, Montague, \n",
            "Stay this another crown. \n",
            "\n",
            "WARWICK: \n",
            "It is your will, my wife a king, and, all \n",
            "The suit of you. \n",
            "My Lord of Warwick, what is the hand of York \n",
            "My gracious lord, be by your spirits: our foes are nigh, \n",
            "And this soft courage makes your followers faint. \n",
            "You promised knighthood to our forward son: \n",
            "Unsheathe your sword, and dub him presently. \n",
            "Edward, kneel down. \n",
            "\n",
            "KING HENRY VI: \n",
            "Ay, messenger, what letters \n",
            "KING EDWARD IV: \n",
            "What, will you now? these good words to them will do him wrong, \n",
            "And make that nature does to say you must. \n",
            "If Henry's enemies? \n",
            "O Phoebus, hadst thou never given consent \n",
            "That Phaethon should cheque thy fiery steeds, \n",
            "Thy burning car never had scorch'd the earth! \n",
            "And, Henry, hadst thou sway'd as kings were such a man. \n",
            "\n",
            "ARCHIDAMUS: \n",
            "Would they but be won yet? \n",
            "\n",
            "QUEEN MARGARET: \n",
            "Hold, me, hold; for we have done too much. \n",
            "\n",
            "GLOUCESTER: \n",
            "Murder'd her kinsman. O, tell me, friar, tell me, \n",
            "In what vile part of this anatomy \n",
            "Doth my name lodge? tell me, that I may He's \n",
            "Without her, follows Coriolanus. \n",
            "\n",
            "GLOUCESTER: \n",
            "Ay, my good lord:--my lord, I should say rather; \n",
            "'Tis sin to flatter; 'good' was little better: \n",
            "'Good Gloucester' and 'good devil' were alike, \n",
            "And both preposterous; therefore, not 'good lord.' \n",
            "\n",
            "GLOUCESTER: \n",
            "Sirrah, leave us to ourselves: we must confer. \n",
            "\n",
            "KING HENRY VI: \n",
            "So flies the reckless shepherd from the wolf; \n",
            "So first the harmless sheep doth yield his fleece \n",
            "And next his throat unto the butcher's knife. \n",
            "What scene of death hath Roscius now to act? \n",
            "\n",
            "GLOUCESTER: \n",
            "Suspicion always haunts the guilty mind; \n",
            "The thief doth fear each bush an officer. \n",
            "\n",
            "KING HENRY VI: \n",
            "The bird that hath been limed in a bush, \n",
            "With trembling wings misdoubteth every bush; \n",
            "And I, the hapless male to one sweet bird, \n",
            "Have now the fatal object in my eye \n",
            "Where my poor young was limed, was caught and kill'd. \n",
            "\n",
            "GLOUCESTER: \n",
            "Why, what a peevish fool was that of Crete, \n",
            "That taught his son the office of a fowl! \n",
            "An yet, for all his wings, the fool was drown'd. \n",
            "\n",
            "KING HENRY VI: \n",
            "I, Daedalus; my poor boy, Icarus; \n",
            "Thy father, Minos, that denied our course; \n",
            "The sun that sear'd the wings of my sweet boy \n",
            "Thy state and seat is slow to our English lady's nose \n",
            "That has been blue, but not her eyebrows. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLR_S72lPhuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}