{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_shakespeare.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbUyEL-EEGn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN _  many to many (equal size)\n",
        "#I used teacher forcing and didn't consider schadule sampling, for more robust results, you should implement schadule sampling! \n",
        "#import packages\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1476pwvEJjX",
        "colab_type": "code",
        "outputId": "fcc00769-6934-47ca-a079-c8e609143477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#getting datase with tensorflow\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdMOTSxTEL4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO-F8VhJEU2q",
        "colab_type": "code",
        "outputId": "422cabdf-ec93-422f-d458-bbef65973172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(\"Number of words in the corpus: \",len(corpus))\n",
        "text=corpus.splitlines()\n",
        "print(\"Number of lines in the corpus: \",len(text))\n",
        "\n",
        "print(\"The first line of corpus is => \",text[0])\n",
        "# create train, test and validation set\n",
        "for i in range(len(text)):\n",
        "  text[i]=text[i]+\" <end>\" ## adding <end> at end of each line \n",
        "print(\"Now the first line of corpus is => \",text[0]) \n",
        "\n",
        "\n",
        "words=' '.join(text)\n",
        "words=words.split()\n",
        "print(\"printing the 0th to 100th  words of corpus : \",words[0:100])\n",
        "## later, we use these words for creating vocabulary \n",
        "## we dont consider spaces and new lines!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the corpus:  1115394\n",
            "Number of lines in the corpus:  40000\n",
            "The first line of corpus is =>  First Citizen:\n",
            "Now the first line of corpus is =>  First Citizen: <end>\n",
            "printing the 0th to 100th  words of corpus :  ['First', 'Citizen:', '<end>', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', '<end>', '<end>', 'All:', '<end>', 'Speak,', 'speak.', '<end>', '<end>', 'First', 'Citizen:', '<end>', 'You', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish?', '<end>', '<end>', 'All:', '<end>', 'Resolved.', 'resolved.', '<end>', '<end>', 'First', 'Citizen:', '<end>', 'First,', 'you', 'know', 'Caius', 'Marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people.', '<end>', '<end>', 'All:', '<end>', 'We', \"know't,\", 'we', \"know't.\", '<end>', '<end>', 'First', 'Citizen:', '<end>', 'Let', 'us', 'kill', 'him,', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', 'price.', '<end>', \"Is't\", 'a', 'verdict?', '<end>', '<end>', 'All:', '<end>', 'No', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done:', 'away,', 'away!', '<end>', '<end>', 'Second']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ga3Wtuq0xMx",
        "colab_type": "code",
        "outputId": "5d6d73e5-505b-40b8-8542-bf6709c82649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#create vocabulary\n",
        "\n",
        "voc=[]\n",
        "\n",
        "def return_index(word):\n",
        "  return voc.index(word)\n",
        " \n",
        "for word in words :\n",
        "  if word not in voc:\n",
        "    voc.append(word)\n",
        "    \n",
        "print(\"The index of  word (The) in vocabulary: \",return_index('The') )\n",
        "print(\"The word for index (203): \",voc[203] )\n",
        "\n",
        "print(\"Vocab lenght: \",len(voc))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The index of  word (The) in vocabulary:  203\n",
            "The word for index (203):  The\n",
            "Vocab lenght:  25671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJxYsGfx7OIT",
        "colab_type": "code",
        "outputId": "76008240-40f6-4d0b-bbb1-b42321dd64d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# converting words to indexes and save it in words_indexes\n",
        "words_indexes=[return_index(word) for word in words]\n",
        "print(\"printing some first indexes : \",words_indexes[0:100])\n",
        "## its our network input! we use words_indexes  to feed the network\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing some first indexes :  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 11, 2, 12, 10, 2, 2, 0, 1, 2, 13, 14, 15, 16, 17, 18, 19, 20, 18, 21, 2, 2, 11, 2, 22, 23, 2, 2, 0, 1, 2, 24, 25, 26, 27, 28, 29, 30, 31, 18, 32, 33, 2, 2, 11, 2, 34, 35, 4, 36, 2, 2, 0, 1, 2, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 2, 49, 50, 51, 2, 2, 11, 2, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 2, 2, 62]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cjjZtu9-0o0",
        "colab_type": "code",
        "outputId": "acce475c-0cd9-4874-f249-7e4059d1077c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "seq_lenght=35 # the lenght of rnn (the lenght of input)\n",
        "batch_size=20\n",
        "# how many batches(with size: [batch_size, seq_lenght])  we have for words_indexes array ?\n",
        "# the answer is len(words_indexes/(batch_size*seq_lenght))\n",
        "max_batches=int(len(words_indexes)/(batch_size*seq_lenght))\n",
        "print(\"the maximum numbers of batcheswith these setting(batch_size,seq_lenght) :  \",max_batches)\n",
        "\n",
        "\n",
        "\n",
        "trainsetX=words_indexes[0:int(0.9*max_batches)*batch_size*seq_lenght]\n",
        "trainsetY=words_indexes[1:int(0.9*max_batches)*batch_size*seq_lenght+1]\n",
        "validsetX=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght:int(1*max_batches)*batch_size*seq_lenght]\n",
        "validsetY=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght + 1:int(1*max_batches)*batch_size*seq_lenght +1]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the maximum numbers of batcheswith these setting(batch_size,seq_lenght) :   346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJqJCYHp0x_0",
        "colab_type": "code",
        "outputId": "0b4f1f59-32fb-44a9-9ee7-43ab29982983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "##Creating batches\n",
        "\n",
        "def create_batch(data,flag):\n",
        "  if flag=='Train':\n",
        "     return (np.array(data).reshape(int(0.9*max_batches) ,batch_size ,seq_lenght)) # shape(numer of batch, batch_size,seq_enght)\n",
        "  else :\n",
        "     return (np.array(data).reshape(int(1*max_batches)-int(0.9*max_batches) ,batch_size ,seq_lenght)) # shape(numer of batch, batch_size,seq_enght)\n",
        "  \n",
        "trainX_batches =create_batch(trainsetX,'Train')\n",
        "trainY_batches =create_batch(trainsetY,'Train')\n",
        "\n",
        "validX_batches =create_batch(validsetX,'Valid')\n",
        "validY_batches =create_batch(validsetY,'Valid')\n",
        "\n",
        "\n",
        "print(\"trainX_batches_shape: \",trainX_batches.shape)\n",
        "print(\"trainY_batches_shape: \",trainY_batches.shape)\n",
        "\n",
        "print(\"validX_batches_shape: \",validX_batches.shape)\n",
        "print(\"validY_batches_shape: \",validY_batches.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainX_batches_shape:  (311, 20, 35)\n",
            "trainY_batches_shape:  (311, 20, 35)\n",
            "validX_batches_shape:  (35, 20, 35)\n",
            "validY_batches_shape:  (35, 20, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaCR9Gi0DEv0",
        "colab_type": "code",
        "outputId": "c1810e31-6614-49c7-bba3-f165f9a24dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(trainX_batches[0,0,:]) # batchNumber=0 , 0th sample in the batch , all of the seq lenght\n",
        "print(trainY_batches[0,0,:])\n",
        "print(len(words_indexes))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10  2  2 11  2 12 10  2  2  0  1  2 13 14\n",
            " 15 16 17 18 19 20 18 21  2  2 11]\n",
            "[ 1  2  3  4  5  6  7  8  9 10  2  2 11  2 12 10  2  2  0  1  2 13 14 15\n",
            " 16 17 18 19 20 18 21  2  2 11  2]\n",
            "242651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBT0GZrS1IoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now pack these information into one function\n",
        "def get_batches():# use in training \n",
        "  global  words_indexes\n",
        "  a=[]\n",
        "  b=[]\n",
        "    ## shuffling  words_indexes\n",
        "  a=words_indexes[0:int(0.2*len(words_indexes))]\n",
        "  b=words_indexes[int(0.2*len(words_indexes)):]\n",
        "  words_indexes=b+a\n",
        "\n",
        "  trainsetX=words_indexes[0:int(0.9*max_batches)*batch_size*seq_lenght]\n",
        "  trainsetY=words_indexes[1:int(0.9*max_batches)*batch_size*seq_lenght+1]\n",
        "  validsetX=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght:int(1*max_batches)*batch_size*seq_lenght]\n",
        "  validsetY=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght + 1:int(1*max_batches)*batch_size*seq_lenght +1]\n",
        "\n",
        "  trainX_batches =create_batch(trainsetX,'Train')\n",
        "  trainY_batches =create_batch(trainsetY,'Train')\n",
        "  validX_batches =create_batch(validsetX,'Valid')\n",
        "  validY_batches =create_batch(validsetY,'Valid')\n",
        "  \n",
        "  return  trainX_batches,trainY_batches,validX_batches,validY_batches\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gPNo1yy1I2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a network\n",
        "\n",
        "class net(torch.nn.Module):\n",
        "  def  __init__(self,vocab_size,embed_size,hidden_size,number_of_layers,num_dir):\n",
        "    super().__init__()\n",
        "    \n",
        "\n",
        "  \n",
        "    #If the RNN is bidirectional,num_directions should be 2, else it should be 1.\n",
        "  \n",
        "    self.num_directions=num_dir\n",
        "    self.number_of_layers=number_of_layers\n",
        "    self.hidden_size=hidden_size\n",
        "\n",
        "    \n",
        "    \n",
        "    self.embd= torch.nn.Embedding(vocab_size, embed_size)\n",
        "    self.drop1=torch.nn.Dropout(0.3)\n",
        "    self.gru = torch.nn.GRU(embed_size, hidden_size, number_of_layers, dropout=0.4,bidirectional=False)\n",
        "    self.drop2=torch.nn.Dropout(0.1)\n",
        "    self.fc=torch.nn.Linear(hidden_size,vocab_size)\n",
        "    self.init_weights()\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "  def forward(self,x,hidden):\n",
        "    # x shape : torch.Size([seq_lenght, batch_size]) contains index of words in vocab\n",
        "    # hidden shape :torch.Size([num_layers * num_directions, batch_size, hidden_size])\n",
        "    # hidden means h0\n",
        "    \n",
        "    \n",
        "    emb=self.embd(x) \n",
        "    # emb shape : torch.Size([seq_lenght, batch_size, embed_size])\n",
        "    out=self.drop1(emb)\n",
        "    out , hidden =self.gru(emb,hidden) #  out = the toppest  hidden layer for all time steps |  hidden= contains hidden of all hidden layers of last time step\n",
        "    # out shape :  torch.Size([seq_lenght, batch_size, hidden_size  * num_directions])\n",
        "    # hidden shape : torch.Size([num_layers * num_directions, batch_size, hidden_size])\n",
        "    \n",
        "    \n",
        "    out=self.drop2(out) # it doesnt  change the dimension\n",
        "   \n",
        "    output=out.view(out.size(0)* out.size(1),out.size(2))   \n",
        "    # out shape:  torch.Size([seq_lenght * batch_size, hidden_size  * num_directions])\n",
        "    \n",
        "    \n",
        "    output=self.fc(output)\n",
        "    # output shape:  torch.Size([seq_lenght * batch_size, vocab_size])\n",
        "    \n",
        "    \n",
        "    output=output.view(out.shape[0], out.shape[1],output.shape[1])   \n",
        "    # output shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "\n",
        "    \n",
        "    # hidden is h0 for next feeding\n",
        "    return output , hidden\n",
        "  def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embd.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "  def init_hidden(self, batch_size): # initialization with zeroes\n",
        "        weight = next(self.parameters()).data\n",
        "        return torch.autograd.Variable(weight.new(self.number_of_layers, batch_size, self.hidden_size).zero_())\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stvpINrJ1JGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(voc)\n",
        "embed_size=250\n",
        "hidden_size=200\n",
        "number_of_layers=2\n",
        "num_direction=1\n",
        "learning_rate=0.001\n",
        "clip = 0.25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AANGMmnTQjw0",
        "colab_type": "code",
        "outputId": "74fe6e53-824a-4afa-9553-5457115a6258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model=net(vocab_size,embed_size,hidden_size,number_of_layers,num_direction)\n",
        "print(model)\n",
        "if cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net(\n",
            "  (embd): Embedding(25671, 250)\n",
            "  (drop1): Dropout(p=0.3)\n",
            "  (gru): GRU(250, 200, num_layers=2, dropout=0.4)\n",
            "  (drop2): Dropout(p=0.1)\n",
            "  (fc): Linear(in_features=200, out_features=25671, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTgsiNIyQls0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtQ1I_Kw5HHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculating accuracy\n",
        "\n",
        "def cal_accuracy(pred_classes,real_classes):#  shape -> both : [number of test samples,1]  or both : [number of test samples,]\n",
        "  bool_array=(pred_classes==real_classes) # example: bool_array=[True, False, True, False, True]\n",
        "  True_pred_counts=np.count_nonzero(bool_array) # count number of Trues in [True, False, True, False, True]\n",
        "  \n",
        "  return True_pred_counts/pred_classes.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap8AXWsIV1ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# important notices:\n",
        "# a.shape=(2,3,4)\n",
        "# a.rehspae(6,4) -> (a000) (a001) (a002) (a003)      ,a021=a[0,2,1]\n",
        "#                   (a010) (a011) (a012) (a013) \n",
        "#                   (a020) (a021) (a022) (a023) \n",
        "#                   (a100) (a101) (a102) (a103) \n",
        "#                   (a110) (a111) (a112) (a113) \n",
        "#                   (a120) (a121) (a122) (a123) \n",
        "#\n",
        "#\n",
        "# a.shape=(3,2,4)\n",
        "# a.rehspae(6,4) -> (a000) (a001) (a002) (a003) \n",
        "#                   (a010) (a011) (a012) (a013) \n",
        "#                   (a100) (a101) (a102) (a103) \n",
        "#                   (a110) (a111) (a112) (a113) \n",
        "#                   (a200) (a201) (a202) (a203) \n",
        "#                   (a210) (a211) (a212) (a213) \n",
        "#\n",
        "#\n",
        "# a.shape=(3,2)\n",
        "# a.rehspae(6,1) -> (a00) \n",
        "#                   (a01)  \n",
        "#                   (a10)                   \n",
        "#                   (a11)  \n",
        "#                   (a20)  \n",
        "#                   (a21) \n",
        "#\n",
        "#\n",
        "# a.shape=(2,3)\n",
        "# a.rehspae(6,1) -> (a00) \n",
        "#                   (a01)  \n",
        "#                   (a02)                   \n",
        "#                   (a10)  \n",
        "#                   (a11)  \n",
        "#                   (a12) \n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a0rmFKiQmXG",
        "colab_type": "code",
        "outputId": "cddc23ca-71dc-4816-97db-3a73bf109787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs=300\n",
        "mean_train_acc=[]\n",
        "mean_valid_acc=[]\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  validacc=[]\n",
        "  trainacc=[]\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  total_loss=0\n",
        "  model.train()\n",
        "  trainX_batches,trainY_batches,_,_=get_batches()\n",
        "  for batchNumber in range(trainX_batches.shape[0]):\n",
        "    \n",
        "    X=torch.autograd.Variable(torch.LongTensor(trainX_batches[batchNumber].T)) #train_batches[batchNumber].T  shape :nparray ([seq_lenght,batch_size]) \n",
        "    Y=torch.autograd.Variable(torch.LongTensor(trainY_batches[batchNumber].T.reshape(seq_lenght*batch_size)))  #Y:Torch.szie ([seq_lenght*batch_size]) => seq0batch0  ,  seq0batch1 ,  seq0batch2  , ..... \n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "            Y = Y.cuda()\n",
        "     \n",
        "\n",
        "    hidden = torch.autograd.Variable(hidden)#seperating  hidden state from one iteration to another\n",
        "    # because we want to use this new hidden for computational graph, not preivous hiddens\n",
        "   \n",
        "    if cuda.is_available():\n",
        "             hidden = hidden.cuda()\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs,hidden=model(X,hidden)# outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(seq_lenght*batch_size, vocab_size)\n",
        "    # outputs shape:  torch.Size([seq_lenght * batch_size, vocab_size])=> seq0batch0,vocab  ,  seq0batch1,vocab ,  seq0batch2,vocab  , .....\n",
        "    \n",
        "    loss=criterion(outputs,Y)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    pred=outputs\n",
        "    pred_classes=torch.max(pred, 1)[1]# pred_classes.shape : torch.Size([seq_lenght * batch_size])\n",
        "    acc=cal_accuracy(np.array(pred_classes.cpu()),Y.cpu().numpy()) \n",
        "    trainacc.append(acc*100)\n",
        "\n",
        "  print(\"training accuracy in epoch: \",i,\" is \",np.mean(trainacc))\n",
        "  mean_train_acc.append(np.mean(trainacc))\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  model.eval() \n",
        "  _,_,validX_batches,validY_batches=get_batches()\n",
        "\n",
        "  for batchNumber in range(validX_batches.shape[0]):\n",
        "    \n",
        "    X=torch.autograd.Variable(torch.LongTensor(validX_batches[batchNumber].T)) #valid_batches[batchNumber].T  shape :nparray ([seq_lenght,batch_size]) \n",
        "    Y=torch.autograd.Variable(torch.LongTensor(validY_batches[batchNumber].T.reshape(seq_lenght*batch_size)))  #Y:Torch.szie ([seq_lenght*batch_size]) \n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "            Y = Y.cuda()\n",
        "  \n",
        "    if cuda.is_available():\n",
        "            hidden = hidden.cuda()\n",
        "      \n",
        "    outputs,hidden=model(X,hidden)# outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(seq_lenght*batch_size, vocab_size)\n",
        "    # outputs shape:  torch.Size([seq_lenght * batch_size, vocab_size])\n",
        "    pred=outputs \n",
        "    pred_classes=torch.max(pred.data, 1)[1]# torch.Size([seq_lenght * batch_size])\n",
        "    acc=cal_accuracy(np.array(pred_classes.cpu()),Y.cpu().numpy())\n",
        "    validacc.append(acc*100)\n",
        "\n",
        "  print(\"validation accuracy in epoch: \",i,\" is \",np.mean(validacc))\n",
        "  mean_valid_acc.append(np.mean(validacc))\n",
        "\n",
        "  "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training accuracy in epoch:  0  is  21.805695911805238\n",
            "validation accuracy in epoch:  0  is  22.1265306122449\n",
            "training accuracy in epoch:  1  is  23.065227377124483\n",
            "validation accuracy in epoch:  1  is  23.673469387755105\n",
            "training accuracy in epoch:  2  is  23.4680753330271\n",
            "validation accuracy in epoch:  2  is  22.591836734693874\n",
            "training accuracy in epoch:  3  is  23.953146531924666\n",
            "validation accuracy in epoch:  3  is  22.322448979591837\n",
            "training accuracy in epoch:  4  is  24.421221864951768\n",
            "validation accuracy in epoch:  4  is  23.99183673469388\n",
            "training accuracy in epoch:  5  is  25.141938447404684\n",
            "validation accuracy in epoch:  5  is  25.044897959183675\n",
            "training accuracy in epoch:  6  is  25.966008268259074\n",
            "validation accuracy in epoch:  6  is  25.98775510204082\n",
            "training accuracy in epoch:  7  is  26.462563160312357\n",
            "validation accuracy in epoch:  7  is  24.281632653061227\n",
            "training accuracy in epoch:  8  is  27.380799265043642\n",
            "validation accuracy in epoch:  8  is  24.26530612244898\n",
            "training accuracy in epoch:  9  is  28.040422599908133\n",
            "validation accuracy in epoch:  9  is  26.86122448979592\n",
            "training accuracy in epoch:  10  is  28.977951309141023\n",
            "validation accuracy in epoch:  10  is  27.522448979591836\n",
            "training accuracy in epoch:  11  is  30.21175930179145\n",
            "validation accuracy in epoch:  11  is  27.534693877551017\n",
            "training accuracy in epoch:  12  is  30.977032613688557\n",
            "validation accuracy in epoch:  12  is  26.57551020408163\n",
            "training accuracy in epoch:  13  is  32.005512172714745\n",
            "validation accuracy in epoch:  13  is  26.028571428571432\n",
            "training accuracy in epoch:  14  is  32.98392282958199\n",
            "validation accuracy in epoch:  14  is  29.730612244897955\n",
            "training accuracy in epoch:  15  is  33.957740009186956\n",
            "validation accuracy in epoch:  15  is  29.465306122448983\n",
            "training accuracy in epoch:  16  is  35.05190629306385\n",
            "validation accuracy in epoch:  16  is  32.26530612244898\n",
            "training accuracy in epoch:  17  is  35.99173174092788\n",
            "validation accuracy in epoch:  17  is  30.273469387755103\n",
            "training accuracy in epoch:  18  is  36.746440055121724\n",
            "validation accuracy in epoch:  18  is  27.8734693877551\n",
            "training accuracy in epoch:  19  is  37.52503445107946\n",
            "validation accuracy in epoch:  19  is  33.52244897959184\n",
            "training accuracy in epoch:  20  is  38.839228295819936\n",
            "validation accuracy in epoch:  20  is  31.326530612244902\n",
            "training accuracy in epoch:  21  is  39.7156637574644\n",
            "validation accuracy in epoch:  21  is  34.36326530612244\n",
            "training accuracy in epoch:  22  is  40.521359669269636\n",
            "validation accuracy in epoch:  22  is  32.089795918367344\n",
            "training accuracy in epoch:  23  is  41.09784106568672\n",
            "validation accuracy in epoch:  23  is  30.96734693877551\n",
            "training accuracy in epoch:  24  is  41.51446945337621\n",
            "validation accuracy in epoch:  24  is  34.95510204081632\n",
            "training accuracy in epoch:  25  is  42.77675700505283\n",
            "validation accuracy in epoch:  25  is  33.68571428571428\n",
            "training accuracy in epoch:  26  is  43.55902618282039\n",
            "validation accuracy in epoch:  26  is  36.293877551020415\n",
            "training accuracy in epoch:  27  is  44.331649058337156\n",
            "validation accuracy in epoch:  27  is  37.47755102040816\n",
            "training accuracy in epoch:  28  is  44.66192007349563\n",
            "validation accuracy in epoch:  28  is  34.21224489795919\n",
            "training accuracy in epoch:  29  is  45.043178686265506\n",
            "validation accuracy in epoch:  29  is  37.68571428571428\n",
            "training accuracy in epoch:  30  is  46.140560404226\n",
            "validation accuracy in epoch:  30  is  36.30612244897959\n",
            "training accuracy in epoch:  31  is  46.95957740009187\n",
            "validation accuracy in epoch:  31  is  39.75918367346939\n",
            "training accuracy in epoch:  32  is  47.428112080845196\n",
            "validation accuracy in epoch:  32  is  40.95918367346939\n",
            "training accuracy in epoch:  33  is  47.72347266881029\n",
            "validation accuracy in epoch:  33  is  35.82857142857143\n",
            "training accuracy in epoch:  34  is  48.152503445107946\n",
            "validation accuracy in epoch:  34  is  40.63673469387756\n",
            "training accuracy in epoch:  35  is  49.0032154340836\n",
            "validation accuracy in epoch:  35  is  41.21632653061225\n",
            "training accuracy in epoch:  36  is  49.989894350022965\n",
            "validation accuracy in epoch:  36  is  41.2530612244898\n",
            "training accuracy in epoch:  37  is  50.50711988975654\n",
            "validation accuracy in epoch:  37  is  46.583673469387755\n",
            "training accuracy in epoch:  38  is  50.82085438677078\n",
            "validation accuracy in epoch:  38  is  41.3469387755102\n",
            "training accuracy in epoch:  39  is  50.989894350022965\n",
            "validation accuracy in epoch:  39  is  47.032653061224494\n",
            "training accuracy in epoch:  40  is  51.8626550298576\n",
            "validation accuracy in epoch:  40  is  44.28571428571428\n",
            "training accuracy in epoch:  41  is  52.63849333945796\n",
            "validation accuracy in epoch:  41  is  44.03265306122448\n",
            "training accuracy in epoch:  42  is  52.98346348185577\n",
            "validation accuracy in epoch:  42  is  50.28979591836734\n",
            "training accuracy in epoch:  43  is  53.29627928341754\n",
            "validation accuracy in epoch:  43  is  43.057142857142864\n",
            "training accuracy in epoch:  44  is  53.56637574644006\n",
            "validation accuracy in epoch:  44  is  49.24489795918368\n",
            "training accuracy in epoch:  45  is  54.265043638033994\n",
            "validation accuracy in epoch:  45  is  47.24489795918368\n",
            "training accuracy in epoch:  46  is  54.99632521819018\n",
            "validation accuracy in epoch:  46  is  46.665306122448975\n",
            "training accuracy in epoch:  47  is  55.19154800183739\n",
            "validation accuracy in epoch:  47  is  53.563265306122446\n",
            "training accuracy in epoch:  48  is  55.52411575562701\n",
            "validation accuracy in epoch:  48  is  47.179591836734694\n",
            "training accuracy in epoch:  49  is  55.59898943500231\n",
            "validation accuracy in epoch:  49  is  53.68163265306123\n",
            "training accuracy in epoch:  50  is  56.25999081304548\n",
            "validation accuracy in epoch:  50  is  51.05714285714285\n",
            "training accuracy in epoch:  51  is  56.89940284795589\n",
            "validation accuracy in epoch:  51  is  49.78775510204081\n",
            "training accuracy in epoch:  52  is  57.12356453835554\n",
            "validation accuracy in epoch:  52  is  52.632653061224495\n",
            "training accuracy in epoch:  53  is  57.668350941662844\n",
            "validation accuracy in epoch:  53  is  47.95102040816327\n",
            "training accuracy in epoch:  54  is  57.626090950849786\n",
            "validation accuracy in epoch:  54  is  52.48571428571429\n",
            "training accuracy in epoch:  55  is  58.36564079007809\n",
            "validation accuracy in epoch:  55  is  55.43265306122449\n",
            "training accuracy in epoch:  56  is  59.01837390904914\n",
            "validation accuracy in epoch:  56  is  52.159183673469386\n",
            "training accuracy in epoch:  57  is  59.17225539733578\n",
            "validation accuracy in epoch:  57  is  58.824489795918375\n",
            "training accuracy in epoch:  58  is  59.39687643546165\n",
            "validation accuracy in epoch:  58  is  52.12653061224489\n",
            "training accuracy in epoch:  59  is  59.395957740009194\n",
            "validation accuracy in epoch:  59  is  58.600000000000016\n",
            "training accuracy in epoch:  60  is  60.10932475884244\n",
            "validation accuracy in epoch:  60  is  56.68571428571428\n",
            "training accuracy in epoch:  61  is  60.71428571428571\n",
            "validation accuracy in epoch:  61  is  52.06938775510204\n",
            "training accuracy in epoch:  62  is  60.63435920992192\n",
            "validation accuracy in epoch:  62  is  64.28571428571429\n",
            "training accuracy in epoch:  63  is  61.093706936150674\n",
            "validation accuracy in epoch:  63  is  51.39591836734694\n",
            "training accuracy in epoch:  64  is  61.11483693155718\n",
            "validation accuracy in epoch:  64  is  57.673469387755105\n",
            "training accuracy in epoch:  65  is  61.69637115296279\n",
            "validation accuracy in epoch:  65  is  59.987755102040815\n",
            "training accuracy in epoch:  66  is  62.41433164905835\n",
            "validation accuracy in epoch:  66  is  56.28163265306122\n",
            "training accuracy in epoch:  67  is  62.3307303628847\n",
            "validation accuracy in epoch:  67  is  66.51836734693877\n",
            "training accuracy in epoch:  68  is  62.66605420303169\n",
            "validation accuracy in epoch:  68  is  56.40408163265307\n",
            "training accuracy in epoch:  69  is  62.75516766192007\n",
            "validation accuracy in epoch:  69  is  61.8408163265306\n",
            "training accuracy in epoch:  70  is  63.13780431786862\n",
            "validation accuracy in epoch:  70  is  62.15510204081633\n",
            "training accuracy in epoch:  71  is  63.89572806614607\n",
            "validation accuracy in epoch:  71  is  59.38367346938776\n",
            "training accuracy in epoch:  72  is  63.71474506201195\n",
            "validation accuracy in epoch:  72  is  68.15510204081633\n",
            "training accuracy in epoch:  73  is  64.0349104271934\n",
            "validation accuracy in epoch:  73  is  58.00816326530612\n",
            "training accuracy in epoch:  74  is  63.949012402388604\n",
            "validation accuracy in epoch:  74  is  68.00816326530614\n",
            "training accuracy in epoch:  75  is  64.57096922370235\n",
            "validation accuracy in epoch:  75  is  62.77959183673469\n",
            "training accuracy in epoch:  76  is  65.17041800643086\n",
            "validation accuracy in epoch:  76  is  60.8408163265306\n",
            "training accuracy in epoch:  77  is  65.28709232889295\n",
            "validation accuracy in epoch:  77  is  68.82857142857142\n",
            "training accuracy in epoch:  78  is  65.44878272852549\n",
            "validation accuracy in epoch:  78  is  58.522448979591836\n",
            "training accuracy in epoch:  79  is  65.28801102434542\n",
            "validation accuracy in epoch:  79  is  68.80000000000001\n",
            "training accuracy in epoch:  80  is  65.63114377583831\n",
            "validation accuracy in epoch:  80  is  68.42857142857143\n",
            "training accuracy in epoch:  81  is  66.62241616903997\n",
            "validation accuracy in epoch:  81  is  61.751020408163264\n",
            "training accuracy in epoch:  82  is  66.51998162609095\n",
            "validation accuracy in epoch:  82  is  72.02857142857141\n",
            "training accuracy in epoch:  83  is  66.6054203031695\n",
            "validation accuracy in epoch:  83  is  60.730612244897955\n",
            "training accuracy in epoch:  84  is  66.55167661920073\n",
            "validation accuracy in epoch:  84  is  67.38367346938776\n",
            "training accuracy in epoch:  85  is  66.88240698208544\n",
            "validation accuracy in epoch:  85  is  68.9061224489796\n",
            "training accuracy in epoch:  86  is  67.87965089572806\n",
            "validation accuracy in epoch:  86  is  65.11836734693877\n",
            "training accuracy in epoch:  87  is  67.60358291226459\n",
            "validation accuracy in epoch:  87  is  73.02857142857144\n",
            "training accuracy in epoch:  88  is  67.85392742305925\n",
            "validation accuracy in epoch:  88  is  63.50204081632653\n",
            "training accuracy in epoch:  89  is  67.72301332108405\n",
            "validation accuracy in epoch:  89  is  65.10612244897959\n",
            "training accuracy in epoch:  90  is  67.98346348185575\n",
            "validation accuracy in epoch:  90  is  71.62448979591838\n",
            "training accuracy in epoch:  91  is  68.87459807073955\n",
            "validation accuracy in epoch:  91  is  63.857142857142854\n",
            "training accuracy in epoch:  92  is  68.78870004593477\n",
            "validation accuracy in epoch:  92  is  74.94285714285714\n",
            "training accuracy in epoch:  93  is  69.07257694074414\n",
            "validation accuracy in epoch:  93  is  65.50204081632656\n",
            "training accuracy in epoch:  94  is  68.82912264584292\n",
            "validation accuracy in epoch:  94  is  74.19591836734695\n",
            "training accuracy in epoch:  95  is  69.28801102434542\n",
            "validation accuracy in epoch:  95  is  73.27755102040815\n",
            "training accuracy in epoch:  96  is  69.75792374827745\n",
            "validation accuracy in epoch:  96  is  68.05306122448981\n",
            "training accuracy in epoch:  97  is  69.88745980707395\n",
            "validation accuracy in epoch:  97  is  75.28163265306122\n",
            "training accuracy in epoch:  98  is  69.85117133670188\n",
            "validation accuracy in epoch:  98  is  65.64489795918368\n",
            "training accuracy in epoch:  99  is  69.77905374368397\n",
            "validation accuracy in epoch:  99  is  75.22857142857143\n",
            "training accuracy in epoch:  100  is  70.02021129995407\n",
            "validation accuracy in epoch:  100  is  74.56326530612246\n",
            "training accuracy in epoch:  101  is  70.84244372990354\n",
            "validation accuracy in epoch:  101  is  70.18367346938774\n",
            "training accuracy in epoch:  102  is  70.87459807073955\n",
            "validation accuracy in epoch:  102  is  77.8530612244898\n",
            "training accuracy in epoch:  103  is  70.95590261828205\n",
            "validation accuracy in epoch:  103  is  68.1591836734694\n",
            "training accuracy in epoch:  104  is  70.38768948093707\n",
            "validation accuracy in epoch:  104  is  74.00408163265305\n",
            "training accuracy in epoch:  105  is  70.93569131832797\n",
            "validation accuracy in epoch:  105  is  74.0204081632653\n",
            "training accuracy in epoch:  106  is  71.83096003674783\n",
            "validation accuracy in epoch:  106  is  71.26938775510204\n",
            "training accuracy in epoch:  107  is  71.44694533762058\n",
            "validation accuracy in epoch:  107  is  75.28979591836735\n",
            "training accuracy in epoch:  108  is  71.60909508497933\n",
            "validation accuracy in epoch:  108  is  67.66530612244898\n",
            "training accuracy in epoch:  109  is  71.53559944878272\n",
            "validation accuracy in epoch:  109  is  77.41632653061225\n",
            "training accuracy in epoch:  110  is  71.73679375287092\n",
            "validation accuracy in epoch:  110  is  75.56326530612245\n",
            "training accuracy in epoch:  111  is  72.44235186035829\n",
            "validation accuracy in epoch:  111  is  71.83673469387755\n",
            "training accuracy in epoch:  112  is  72.421681212678\n",
            "validation accuracy in epoch:  112  is  80.09387755102041\n",
            "training accuracy in epoch:  113  is  72.5029857602205\n",
            "validation accuracy in epoch:  113  is  70.11428571428573\n",
            "training accuracy in epoch:  114  is  72.18190169958659\n",
            "validation accuracy in epoch:  114  is  72.05714285714286\n",
            "training accuracy in epoch:  115  is  72.38217730822232\n",
            "validation accuracy in epoch:  115  is  77.79591836734693\n",
            "training accuracy in epoch:  116  is  73.17638952687184\n",
            "validation accuracy in epoch:  116  is  71.50204081632653\n",
            "training accuracy in epoch:  117  is  73.14699127239321\n",
            "validation accuracy in epoch:  117  is  79.14285714285712\n",
            "training accuracy in epoch:  118  is  73.32246210381258\n",
            "validation accuracy in epoch:  118  is  68.43673469387757\n",
            "training accuracy in epoch:  119  is  73.12677997243914\n",
            "validation accuracy in epoch:  119  is  74.62448979591836\n",
            "training accuracy in epoch:  120  is  73.2062471290767\n",
            "validation accuracy in epoch:  120  is  78.54693877551021\n",
            "training accuracy in epoch:  121  is  73.94166283876895\n",
            "validation accuracy in epoch:  121  is  73.9265306122449\n",
            "training accuracy in epoch:  122  is  73.78364722094626\n",
            "validation accuracy in epoch:  122  is  78.8408163265306\n",
            "training accuracy in epoch:  123  is  73.91180523656408\n",
            "validation accuracy in epoch:  123  is  69.51020408163268\n",
            "training accuracy in epoch:  124  is  73.7698667891594\n",
            "validation accuracy in epoch:  124  is  79.19591836734693\n",
            "training accuracy in epoch:  125  is  73.94855305466238\n",
            "validation accuracy in epoch:  125  is  78.65714285714286\n",
            "training accuracy in epoch:  126  is  74.48047772163528\n",
            "validation accuracy in epoch:  126  is  74.7265306122449\n",
            "training accuracy in epoch:  127  is  74.43362425355996\n",
            "validation accuracy in epoch:  127  is  81.05714285714286\n",
            "training accuracy in epoch:  128  is  74.4841525034451\n",
            "validation accuracy in epoch:  128  is  69.87346938775508\n",
            "training accuracy in epoch:  129  is  74.40376665135507\n",
            "validation accuracy in epoch:  129  is  80.29387755102042\n",
            "training accuracy in epoch:  130  is  74.78870004593477\n",
            "validation accuracy in epoch:  130  is  80.57551020408164\n",
            "training accuracy in epoch:  131  is  75.06844281120809\n",
            "validation accuracy in epoch:  131  is  76.3265306122449\n",
            "training accuracy in epoch:  132  is  74.88102893890675\n",
            "validation accuracy in epoch:  132  is  80.05714285714285\n",
            "training accuracy in epoch:  133  is  75.16260909508497\n",
            "validation accuracy in epoch:  133  is  70.42857142857143\n",
            "training accuracy in epoch:  134  is  75.08084519981627\n",
            "validation accuracy in epoch:  134  is  82.2530612244898\n",
            "training accuracy in epoch:  135  is  75.14653192466697\n",
            "validation accuracy in epoch:  135  is  81.02857142857142\n",
            "training accuracy in epoch:  136  is  75.86035829122646\n",
            "validation accuracy in epoch:  136  is  78.80816326530612\n",
            "training accuracy in epoch:  137  is  75.55213596692695\n",
            "validation accuracy in epoch:  137  is  81.90612244897959\n",
            "training accuracy in epoch:  138  is  75.75792374827745\n",
            "validation accuracy in epoch:  138  is  72.00408163265307\n",
            "training accuracy in epoch:  139  is  75.23610473128157\n",
            "validation accuracy in epoch:  139  is  80.99591836734693\n",
            "training accuracy in epoch:  140  is  75.86035829122646\n",
            "validation accuracy in epoch:  140  is  81.4938775510204\n",
            "training accuracy in epoch:  141  is  76.08176389526872\n",
            "validation accuracy in epoch:  141  is  77.62857142857143\n",
            "training accuracy in epoch:  142  is  75.96600826825907\n",
            "validation accuracy in epoch:  142  is  83.73877551020408\n",
            "training accuracy in epoch:  143  is  76.21956821313735\n",
            "validation accuracy in epoch:  143  is  72.56326530612245\n",
            "training accuracy in epoch:  144  is  75.91593936610015\n",
            "validation accuracy in epoch:  144  is  80.13469387755102\n",
            "training accuracy in epoch:  145  is  76.16123105190628\n",
            "validation accuracy in epoch:  145  is  81.55510204081634\n",
            "training accuracy in epoch:  146  is  76.88240698208544\n",
            "validation accuracy in epoch:  146  is  75.25714285714285\n",
            "training accuracy in epoch:  147  is  76.50849793293524\n",
            "validation accuracy in epoch:  147  is  83.46530612244898\n",
            "training accuracy in epoch:  148  is  76.74965548920532\n",
            "validation accuracy in epoch:  148  is  72.24489795918366\n",
            "training accuracy in epoch:  149  is  76.53789618741386\n",
            "validation accuracy in epoch:  149  is  80.32244897959184\n",
            "training accuracy in epoch:  150  is  76.81488286632981\n",
            "validation accuracy in epoch:  150  is  79.87346938775511\n",
            "training accuracy in epoch:  151  is  77.39503904455674\n",
            "validation accuracy in epoch:  151  is  77.73469387755102\n",
            "training accuracy in epoch:  152  is  77.05006890215893\n",
            "validation accuracy in epoch:  152  is  84.19591836734693\n",
            "training accuracy in epoch:  153  is  77.2062471290767\n",
            "validation accuracy in epoch:  153  is  72.42448979591838\n",
            "training accuracy in epoch:  154  is  76.859439595774\n",
            "validation accuracy in epoch:  154  is  79.07755102040817\n",
            "training accuracy in epoch:  155  is  77.1694993109784\n",
            "validation accuracy in epoch:  155  is  80.93469387755104\n",
            "training accuracy in epoch:  156  is  77.76757005052826\n",
            "validation accuracy in epoch:  156  is  79.73061224489797\n",
            "training accuracy in epoch:  157  is  77.60679834634819\n",
            "validation accuracy in epoch:  157  is  84.67346938775509\n",
            "training accuracy in epoch:  158  is  77.56407900780891\n",
            "validation accuracy in epoch:  158  is  74.05306122448981\n",
            "training accuracy in epoch:  159  is  77.319246669729\n",
            "validation accuracy in epoch:  159  is  78.13061224489797\n",
            "training accuracy in epoch:  160  is  77.59485530546624\n",
            "validation accuracy in epoch:  160  is  83.34285714285714\n",
            "training accuracy in epoch:  161  is  78.14790996784566\n",
            "validation accuracy in epoch:  161  is  80.81632653061224\n",
            "training accuracy in epoch:  162  is  78.11391823610474\n",
            "validation accuracy in epoch:  162  is  85.36326530612244\n",
            "training accuracy in epoch:  163  is  77.8001837390905\n",
            "validation accuracy in epoch:  163  is  75.8530612244898\n",
            "training accuracy in epoch:  164  is  77.5668350941663\n",
            "validation accuracy in epoch:  164  is  84.95510204081633\n",
            "training accuracy in epoch:  165  is  77.97841065686725\n",
            "validation accuracy in epoch:  165  is  82.5673469387755\n",
            "training accuracy in epoch:  166  is  78.55856683509417\n",
            "validation accuracy in epoch:  166  is  80.0612244897959\n",
            "training accuracy in epoch:  167  is  78.31832797427653\n",
            "validation accuracy in epoch:  167  is  84.55510204081632\n",
            "training accuracy in epoch:  168  is  78.28892971979788\n",
            "validation accuracy in epoch:  168  is  74.70204081632654\n",
            "training accuracy in epoch:  169  is  78.09049150206705\n",
            "validation accuracy in epoch:  169  is  83.64489795918368\n",
            "training accuracy in epoch:  170  is  78.40009186954524\n",
            "validation accuracy in epoch:  170  is  83.86122448979593\n",
            "training accuracy in epoch:  171  is  78.88608176389526\n",
            "validation accuracy in epoch:  171  is  80.4530612244898\n",
            "training accuracy in epoch:  172  is  78.81350482315112\n",
            "validation accuracy in epoch:  172  is  85.76326530612246\n",
            "training accuracy in epoch:  173  is  78.5094166283877\n",
            "validation accuracy in epoch:  173  is  76.25306122448978\n",
            "training accuracy in epoch:  174  is  78.3881488286633\n",
            "validation accuracy in epoch:  174  is  79.69795918367346\n",
            "training accuracy in epoch:  175  is  78.91042719338539\n",
            "validation accuracy in epoch:  175  is  83.08571428571427\n",
            "training accuracy in epoch:  176  is  79.12723932016537\n",
            "validation accuracy in epoch:  176  is  80.73877551020408\n",
            "training accuracy in epoch:  177  is  79.19981626090951\n",
            "validation accuracy in epoch:  177  is  86.86938775510203\n",
            "training accuracy in epoch:  178  is  79.0252641249426\n",
            "validation accuracy in epoch:  178  is  78.02857142857142\n",
            "training accuracy in epoch:  179  is  78.66559485530547\n",
            "validation accuracy in epoch:  179  is  84.10612244897959\n",
            "training accuracy in epoch:  180  is  79.0096463022508\n",
            "validation accuracy in epoch:  180  is  85.02448979591836\n",
            "training accuracy in epoch:  181  is  79.37758383096003\n",
            "validation accuracy in epoch:  181  is  79.7469387755102\n",
            "training accuracy in epoch:  182  is  79.38493339457969\n",
            "validation accuracy in epoch:  182  is  87.1265306122449\n",
            "training accuracy in epoch:  183  is  79.50022967386312\n",
            "validation accuracy in epoch:  183  is  74.95918367346938\n",
            "training accuracy in epoch:  184  is  79.12494258153423\n",
            "validation accuracy in epoch:  184  is  79.53877551020408\n",
            "training accuracy in epoch:  185  is  79.48874598070739\n",
            "validation accuracy in epoch:  185  is  85.94693877551019\n",
            "training accuracy in epoch:  186  is  79.79513091410197\n",
            "validation accuracy in epoch:  186  is  81.01632653061226\n",
            "training accuracy in epoch:  187  is  79.73679375287092\n",
            "validation accuracy in epoch:  187  is  86.81632653061226\n",
            "training accuracy in epoch:  188  is  79.66513550757924\n",
            "validation accuracy in epoch:  188  is  78.15510204081633\n",
            "training accuracy in epoch:  189  is  79.45475424896647\n",
            "validation accuracy in epoch:  189  is  86.24489795918366\n",
            "training accuracy in epoch:  190  is  79.60633899862196\n",
            "validation accuracy in epoch:  190  is  84.48979591836734\n",
            "training accuracy in epoch:  191  is  80.04042259990813\n",
            "validation accuracy in epoch:  191  is  80.12653061224489\n",
            "training accuracy in epoch:  192  is  80.01102434542949\n",
            "validation accuracy in epoch:  192  is  86.74285714285713\n",
            "training accuracy in epoch:  193  is  79.94304088194764\n",
            "validation accuracy in epoch:  193  is  79.57551020408162\n",
            "training accuracy in epoch:  194  is  79.86541111621499\n",
            "validation accuracy in epoch:  194  is  80.01224489795919\n",
            "training accuracy in epoch:  195  is  79.97657326596233\n",
            "validation accuracy in epoch:  195  is  83.32653061224491\n",
            "training accuracy in epoch:  196  is  80.50666054203033\n",
            "validation accuracy in epoch:  196  is  82.3918367346939\n",
            "training accuracy in epoch:  197  is  80.38631143775838\n",
            "validation accuracy in epoch:  197  is  86.60408163265305\n",
            "training accuracy in epoch:  198  is  80.2411575562701\n",
            "validation accuracy in epoch:  198  is  78.98775510204082\n",
            "training accuracy in epoch:  199  is  80.04547542489664\n",
            "validation accuracy in epoch:  199  is  83.57551020408164\n",
            "training accuracy in epoch:  200  is  80.21819016995866\n",
            "validation accuracy in epoch:  200  is  84.72244897959183\n",
            "training accuracy in epoch:  201  is  80.53927423059255\n",
            "validation accuracy in epoch:  201  is  82.02448979591836\n",
            "training accuracy in epoch:  202  is  80.33716123105191\n",
            "validation accuracy in epoch:  202  is  87.99591836734695\n",
            "training accuracy in epoch:  203  is  80.59761139182362\n",
            "validation accuracy in epoch:  203  is  78.04897959183674\n",
            "training accuracy in epoch:  204  is  80.32475884244373\n",
            "validation accuracy in epoch:  204  is  86.55918367346938\n",
            "training accuracy in epoch:  205  is  80.57556270096462\n",
            "validation accuracy in epoch:  205  is  84.06938775510203\n",
            "training accuracy in epoch:  206  is  80.83830960036748\n",
            "validation accuracy in epoch:  206  is  81.60816326530612\n",
            "training accuracy in epoch:  207  is  80.81488286632981\n",
            "validation accuracy in epoch:  207  is  86.3469387755102\n",
            "training accuracy in epoch:  208  is  80.77400091869545\n",
            "validation accuracy in epoch:  208  is  80.06122448979592\n",
            "training accuracy in epoch:  209  is  80.43592099219109\n",
            "validation accuracy in epoch:  209  is  86.58775510204082\n",
            "training accuracy in epoch:  210  is  80.55672944418926\n",
            "validation accuracy in epoch:  210  is  82.86530612244898\n",
            "training accuracy in epoch:  211  is  80.83463481855765\n",
            "validation accuracy in epoch:  211  is  82.3795918367347\n",
            "training accuracy in epoch:  212  is  80.91042719338539\n",
            "validation accuracy in epoch:  212  is  88.02448979591836\n",
            "training accuracy in epoch:  213  is  80.89067524115755\n",
            "validation accuracy in epoch:  213  is  84.68571428571427\n",
            "training accuracy in epoch:  214  is  80.94212218649518\n",
            "validation accuracy in epoch:  214  is  87.80816326530613\n",
            "training accuracy in epoch:  215  is  80.99586587046396\n",
            "validation accuracy in epoch:  215  is  85.51836734693877\n",
            "training accuracy in epoch:  216  is  81.44327055581076\n",
            "validation accuracy in epoch:  216  is  82.31020408163266\n",
            "training accuracy in epoch:  217  is  81.2397795130914\n",
            "validation accuracy in epoch:  217  is  87.99999999999999\n",
            "training accuracy in epoch:  218  is  81.1231051906293\n",
            "validation accuracy in epoch:  218  is  80.7795918367347\n",
            "training accuracy in epoch:  219  is  80.81212677997244\n",
            "validation accuracy in epoch:  219  is  87.51836734693877\n",
            "training accuracy in epoch:  220  is  81.28847037207164\n",
            "validation accuracy in epoch:  220  is  85.89387755102042\n",
            "training accuracy in epoch:  221  is  81.55994487827286\n",
            "validation accuracy in epoch:  221  is  81.57142857142856\n",
            "training accuracy in epoch:  222  is  81.40836012861736\n",
            "validation accuracy in epoch:  222  is  88.53469387755102\n",
            "training accuracy in epoch:  223  is  81.14928801102435\n",
            "validation accuracy in epoch:  223  is  79.88163265306123\n",
            "training accuracy in epoch:  224  is  81.20486908589803\n",
            "validation accuracy in epoch:  224  is  86.39183673469388\n",
            "training accuracy in epoch:  225  is  81.64124942581535\n",
            "validation accuracy in epoch:  225  is  87.31428571428572\n",
            "training accuracy in epoch:  226  is  81.87919154800184\n",
            "validation accuracy in epoch:  226  is  82.66122448979593\n",
            "training accuracy in epoch:  227  is  81.79145613229214\n",
            "validation accuracy in epoch:  227  is  88.35102040816327\n",
            "training accuracy in epoch:  228  is  81.45613229214516\n",
            "validation accuracy in epoch:  228  is  81.98775510204081\n",
            "training accuracy in epoch:  229  is  81.23426734037668\n",
            "validation accuracy in epoch:  229  is  86.26938775510202\n",
            "training accuracy in epoch:  230  is  81.70831419384474\n",
            "validation accuracy in epoch:  230  is  86.3061224489796\n",
            "training accuracy in epoch:  231  is  82.16536518144235\n",
            "validation accuracy in epoch:  231  is  83.50204081632654\n",
            "training accuracy in epoch:  232  is  81.92834175470831\n",
            "validation accuracy in epoch:  232  is  88.7469387755102\n",
            "training accuracy in epoch:  233  is  81.74230592558567\n",
            "validation accuracy in epoch:  233  is  80.44081632653061\n",
            "training accuracy in epoch:  234  is  81.46577859439597\n",
            "validation accuracy in epoch:  234  is  87.48163265306123\n",
            "training accuracy in epoch:  235  is  81.82820395039045\n",
            "validation accuracy in epoch:  235  is  86.84897959183674\n",
            "training accuracy in epoch:  236  is  82.26779972439137\n",
            "validation accuracy in epoch:  236  is  83.8734693877551\n",
            "training accuracy in epoch:  237  is  82.03628847037207\n",
            "validation accuracy in epoch:  237  is  88.56326530612245\n",
            "training accuracy in epoch:  238  is  81.99310978410658\n",
            "validation accuracy in epoch:  238  is  79.21224489795918\n",
            "training accuracy in epoch:  239  is  81.84611851171337\n",
            "validation accuracy in epoch:  239  is  88.08571428571429\n",
            "training accuracy in epoch:  240  is  82.06660542030318\n",
            "validation accuracy in epoch:  240  is  87.24489795918367\n",
            "training accuracy in epoch:  241  is  82.62333486449242\n",
            "validation accuracy in epoch:  241  is  83.7795918367347\n",
            "training accuracy in epoch:  242  is  82.18098300413413\n",
            "validation accuracy in epoch:  242  is  87.57142857142857\n",
            "training accuracy in epoch:  243  is  82.36334405144694\n",
            "validation accuracy in epoch:  243  is  82.35102040816327\n",
            "training accuracy in epoch:  244  is  81.99402847955902\n",
            "validation accuracy in epoch:  244  is  87.34285714285714\n",
            "training accuracy in epoch:  245  is  82.11437758383096\n",
            "validation accuracy in epoch:  245  is  86.47755102040817\n",
            "training accuracy in epoch:  246  is  82.63068442811208\n",
            "validation accuracy in epoch:  246  is  84.22448979591837\n",
            "training accuracy in epoch:  247  is  82.65319246669729\n",
            "validation accuracy in epoch:  247  is  89.14693877551021\n",
            "training accuracy in epoch:  248  is  82.23426734037668\n",
            "validation accuracy in epoch:  248  is  82.27755102040817\n",
            "training accuracy in epoch:  249  is  82.11851171336703\n",
            "validation accuracy in epoch:  249  is  86.74693877551022\n",
            "training accuracy in epoch:  250  is  82.3945796968305\n",
            "validation accuracy in epoch:  250  is  86.86122448979593\n",
            "training accuracy in epoch:  251  is  82.85117133670188\n",
            "validation accuracy in epoch:  251  is  84.14285714285714\n",
            "training accuracy in epoch:  252  is  82.64676159853008\n",
            "validation accuracy in epoch:  252  is  88.86938775510205\n",
            "training accuracy in epoch:  253  is  82.54524575103355\n",
            "validation accuracy in epoch:  253  is  79.93061224489796\n",
            "training accuracy in epoch:  254  is  82.25310059715204\n",
            "validation accuracy in epoch:  254  is  81.17551020408163\n",
            "training accuracy in epoch:  255  is  82.41065686724849\n",
            "validation accuracy in epoch:  255  is  87.19183673469387\n",
            "training accuracy in epoch:  256  is  82.9490124023886\n",
            "validation accuracy in epoch:  256  is  83.84081632653061\n",
            "training accuracy in epoch:  257  is  82.81120808451999\n",
            "validation accuracy in epoch:  257  is  89.2204081632653\n",
            "training accuracy in epoch:  258  is  82.610473128158\n",
            "validation accuracy in epoch:  258  is  83.07346938775511\n",
            "training accuracy in epoch:  259  is  82.48644924207626\n",
            "validation accuracy in epoch:  259  is  86.44489795918366\n",
            "training accuracy in epoch:  260  is  82.6995865870464\n",
            "validation accuracy in epoch:  260  is  88.0\n",
            "training accuracy in epoch:  261  is  82.93569131832797\n",
            "validation accuracy in epoch:  261  is  84.84489795918367\n",
            "training accuracy in epoch:  262  is  82.8226917776757\n",
            "validation accuracy in epoch:  262  is  89.57142857142856\n",
            "training accuracy in epoch:  263  is  82.8194763435921\n",
            "validation accuracy in epoch:  263  is  80.86938775510203\n",
            "training accuracy in epoch:  264  is  82.58383096003675\n",
            "validation accuracy in epoch:  264  is  88.25714285714287\n",
            "training accuracy in epoch:  265  is  82.81442351860358\n",
            "validation accuracy in epoch:  265  is  87.60408163265305\n",
            "training accuracy in epoch:  266  is  83.11851171336703\n",
            "validation accuracy in epoch:  266  is  84.3673469387755\n",
            "training accuracy in epoch:  267  is  83.09370693615065\n",
            "validation accuracy in epoch:  267  is  87.75510204081631\n",
            "training accuracy in epoch:  268  is  83.05006890215893\n",
            "validation accuracy in epoch:  268  is  82.76734693877553\n",
            "training accuracy in epoch:  269  is  82.8548461185117\n",
            "validation accuracy in epoch:  269  is  88.84489795918367\n",
            "training accuracy in epoch:  270  is  83.07441433164907\n",
            "validation accuracy in epoch:  270  is  87.38775510204081\n",
            "training accuracy in epoch:  271  is  83.22875516766192\n",
            "validation accuracy in epoch:  271  is  84.76326530612245\n",
            "training accuracy in epoch:  272  is  83.24621038125862\n",
            "validation accuracy in epoch:  272  is  88.87755102040816\n",
            "training accuracy in epoch:  273  is  83.16077170418006\n",
            "validation accuracy in epoch:  273  is  84.50204081632653\n",
            "training accuracy in epoch:  274  is  82.84382177308221\n",
            "validation accuracy in epoch:  274  is  81.6857142857143\n",
            "training accuracy in epoch:  275  is  83.1341295360588\n",
            "validation accuracy in epoch:  275  is  87.14693877551021\n",
            "training accuracy in epoch:  276  is  83.37712448323381\n",
            "validation accuracy in epoch:  276  is  84.79183673469387\n",
            "training accuracy in epoch:  277  is  83.30730362884704\n",
            "validation accuracy in epoch:  277  is  88.59591836734694\n",
            "training accuracy in epoch:  278  is  83.37666513550758\n",
            "validation accuracy in epoch:  278  is  84.81632653061224\n",
            "training accuracy in epoch:  279  is  82.88148828663297\n",
            "validation accuracy in epoch:  279  is  88.534693877551\n",
            "training accuracy in epoch:  280  is  83.31694993109784\n",
            "validation accuracy in epoch:  280  is  87.80816326530613\n",
            "training accuracy in epoch:  281  is  83.68213137344969\n",
            "validation accuracy in epoch:  281  is  84.82857142857142\n",
            "training accuracy in epoch:  282  is  83.42903077629767\n",
            "validation accuracy in epoch:  282  is  89.24897959183674\n",
            "training accuracy in epoch:  283  is  83.27285254937989\n",
            "validation accuracy in epoch:  283  is  81.66122448979593\n",
            "training accuracy in epoch:  284  is  83.2379421221865\n",
            "validation accuracy in epoch:  284  is  85.88163265306123\n",
            "training accuracy in epoch:  285  is  83.25815342214055\n",
            "validation accuracy in epoch:  285  is  88.0204081632653\n",
            "training accuracy in epoch:  286  is  83.70969223702345\n",
            "validation accuracy in epoch:  286  is  83.79183673469387\n",
            "training accuracy in epoch:  287  is  83.2462103812586\n",
            "validation accuracy in epoch:  287  is  88.66530612244897\n",
            "training accuracy in epoch:  288  is  83.58934313275151\n",
            "validation accuracy in epoch:  288  is  82.26938775510203\n",
            "training accuracy in epoch:  289  is  83.35002296738631\n",
            "validation accuracy in epoch:  289  is  83.5061224489796\n",
            "training accuracy in epoch:  290  is  83.54294901240239\n",
            "validation accuracy in epoch:  290  is  86.51020408163266\n",
            "training accuracy in epoch:  291  is  83.73036288470372\n",
            "validation accuracy in epoch:  291  is  84.70204081632654\n",
            "training accuracy in epoch:  292  is  83.6789159393661\n",
            "validation accuracy in epoch:  292  is  89.68571428571428\n",
            "training accuracy in epoch:  293  is  83.81488286632981\n",
            "validation accuracy in epoch:  293  is  84.60816326530612\n",
            "training accuracy in epoch:  294  is  83.35875057418467\n",
            "validation accuracy in epoch:  294  is  86.54693877551021\n",
            "training accuracy in epoch:  295  is  83.62976573265964\n",
            "validation accuracy in epoch:  295  is  88.73469387755102\n",
            "training accuracy in epoch:  296  is  83.90307762976573\n",
            "validation accuracy in epoch:  296  is  84.35102040816327\n",
            "training accuracy in epoch:  297  is  83.83279742765274\n",
            "validation accuracy in epoch:  297  is  89.82448979591837\n",
            "training accuracy in epoch:  298  is  83.65778594395957\n",
            "validation accuracy in epoch:  298  is  83.89795918367349\n",
            "training accuracy in epoch:  299  is  83.4492420762517\n",
            "validation accuracy in epoch:  299  is  89.74285714285715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L856xl4UPKUO",
        "colab_type": "code",
        "outputId": "dcad25ea-4990-4b83-b8ce-807304fc5588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot([i+1 for i in range(num_epochs)],mean_valid_acc,label=\"validation accuracy\")\n",
        "plt.plot([i+1 for i in range(num_epochs)],mean_train_acc,label=\"train accuracy\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"Epohs\")\n",
        "plt.ylabel(\"accuracy in % \")\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYMEzeQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nkLwwJWMfTp",
        "colab_type": "code",
        "outputId": "bfd6534d-51df-4b8b-8ad1-61f4cfb50e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#generating words\n",
        "print(\"GENERATING FAKE SHAKESPEARE TEXT: \\n________________________________________________________\")\n",
        "num_words=2000 # th number of words that we want to generate\n",
        "testacc=[]\n",
        "\n",
        "## what is the first word?  answer:  forx example The\n",
        "input_word_index=return_index('The')\n",
        "print('The',end=\" \")\n",
        "X=torch.autograd.Variable(torch.LongTensor(np.array([input_word_index]).reshape(1,1)))\n",
        "hidden = model.init_hidden(1)\n",
        "\n",
        "for i in range(num_words):\n",
        "\n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "           \n",
        "  \n",
        "  \n",
        "#     print(hidden.shape)\n",
        "    outputs,hidden=model(X,hidden)# outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(vocab_size)\n",
        "    outExp=outputs.exp()\n",
        "#     print(outputs)\n",
        "    probablistic_output=(outExp)/(outExp.sum())# probablistic_output shape: torch.Size([vocab_size])\n",
        "#     probablistic_output=((outputs)/(0.4)).exp()\n",
        "\n",
        "    word_index = (torch.multinomial(probablistic_output, 1))\n",
        "    X=torch.autograd.Variable((word_index).reshape(1,1))\n",
        "    if(voc[word_index]==\"<end>\"):\n",
        "      print(\"\")\n",
        "    else:\n",
        "      print(voc[word_index],end=\" \")\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GENERATING FAKE SHAKESPEARE TEXT: \n",
            "________________________________________________________\n",
            "The king! \n",
            "\n",
            "HENRY BOLINGBROKE: \n",
            "Our scene is alter'd from a serious thing, \n",
            "And now changed out storms than day! \n",
            "\n",
            "TYRREL: \n",
            "It is too pause, my lord \n",
            "Before I positively herein: \n",
            "I will resolve your grace immediately. \n",
            "\n",
            "CATESBY: \n",
            "\n",
            "KING RICHARD III: \n",
            "I shall converse with iron-witted fools \n",
            "And unrespective boys: none are for me your best \n",
            "To hear such life I am not yet instructed. \n",
            "\n",
            "ANGELO: \n",
            "'Tis so with me. Wot you what, my lord? \n",
            "To-day the lords you brook the duke inferr'd;' \n",
            "But nothing spake in warrant from himself. \n",
            "When he had done, some followers of mine own, \n",
            "At the lower end of the hall, hurl'd up their caps, \n",
            "And some ten voices cried 'God save King Richard!' \n",
            "And thus I took the vantage of those few, \n",
            "'Thanks, gentle citizens and friends,' quoth I; \n",
            "'This general applause and loving shout \n",
            "Argues your wisdoms and your love to Richard:' \n",
            "And even here brake off, and came away. \n",
            "\n",
            "KING RICHARD III: \n",
            "Well, tell me, my liege, I cannot guess. \n",
            "\n",
            "KING RICHARD III: \n",
            "Well, sir, if you be not, since I know myself \n",
            "And as to speak with me to myself? \n",
            "Methought the souls of all that deceit should dwell \n",
            "In such a daughter of my lord! \n",
            "\n",
            "KING RICHARD II: \n",
            "You were no more than this fashion; lastly, hurried \n",
            "Here to this place, I'll help thee here: \n",
            "My life were better ended by their hate, \n",
            "Than death prorogued, wanting of thy love. \n",
            "\n",
            "THOMAS MOWBRAY: \n",
            "O, but I let it hear your mind herein, \n",
            "You shall not only take the sacrament \n",
            "To bury your intents, but also to effect \n",
            "Whatever I shall happen to devise. \n",
            "I see your brows are full of discontent, \n",
            "Your hearts of sorrow and your hand in war, \n",
            "The tears that can make base youth may he fined \n",
            "For ancient quarrels, and quite lost their hearts. \n",
            "\n",
            "LORD WILLOUGHBY: \n",
            "And daily new exactions are devised, \n",
            "As blanks, benevolences, and I wot not what: \n",
            "But what, o' God's name, doth become of this? \n",
            "\n",
            "NORTHUMBERLAND: \n",
            "Wars have not wasted it, for warr'd he hath not, \n",
            "But basely yielded upon compromise \n",
            "That which his noble ancestors achieved with blows: \n",
            "More hath he spent in peace than nothing in \n",
            "Some consequence yet hanging in the stars \n",
            "Shall bitterly begin his fearful date \n",
            "With this night's revels and expire the term \n",
            "Of a despised life closed in my breast \n",
            "To some daughters; than never with prepare thee: \n",
            "Where is the green lap of fair King Richard's land, \n",
            "My stooping duty tenderly shall show. \n",
            "Go, signify as much, while heaven with nothing rash in any groan, \n",
            "To this rude assault? \n",
            "Villain, I hear better well, if he be deposed? \n",
            "The earth, this tongue of noble noble lord, \n",
            "On Lammas-eve at night shall she be fourteen. \n",
            "Susan and she--God rest all Christian souls!-- \n",
            "Were of an age: well, Susan is with God; \n",
            "She was too good for me; let me prophesy: \n",
            "The blood of English shall manure the ground, \n",
            "And future ages groan for your foul sin. \n",
            "\n",
            "DUKE OF YORK: \n",
            "If thy be gone; turn and under this new spring of time, \n",
            "Lest you prevent have none of thee. \n",
            "Come, go him yesterday, bid time return, \n",
            "And thou shalt be twelve thousand fighting men! \n",
            "To-day, to-day, unhappy day, too late, \n",
            "O'erthrows thy joys, friends, fortune and thy state: \n",
            "For all the Welshmen, hearing you and hold \n",
            "With clog of conscience and sour melancholy \n",
            "Hath yielded up his body to the grave; \n",
            "But here shall doom my death: \n",
            "If thou would give you sleepy drinks, \n",
            "that your senses, unintelligent of our insufficience, \n",
            "may, though they cannot praise us, as little accuse \n",
            "us. \n",
            "\n",
            "NORTHUMBERLAND: \n",
            "Hark, wherefore have they pass'd and pass'd that passing fair? \n",
            "Farewell: thou canst be kinsman, even from my death-bed, thy last leave. \n",
            "In winter's tedious nights sit by the fire \n",
            "With good old folks and let them tell thee tales \n",
            "Of woeful ages long ago betid; \n",
            "And ere thou bid good night, \n",
            "A serpent that will sting thee to the heart. \n",
            "\n",
            "HENRY BOLINGBROKE: \n",
            "O heinous, strong and bold conspiracy! \n",
            "O loyal father of her treacherous son! \n",
            "How long doth save thee? \n",
            "\n",
            "DUKE OF AUMERLE: \n",
            "There's some done. \n",
            "Could stop among these heartless hinds? \n",
            "Turn thee, Benvolio, look upon thy death. \n",
            "\n",
            "BENVOLIO: \n",
            "I do confess another thing to say. \n",
            "Treason! foul treason! Villain! traitor! slave! \n",
            "\n",
            "DUCHESS OF YORK: \n",
            "What is the matter, uncle? speak; \n",
            "Recover breath; tell us how near is danger, \n",
            "That we may arm us to encounter it. \n",
            "\n",
            "DUKE OF YORK: \n",
            "Peruse this writing here, and thou shalt know \n",
            "The treason that my haste forbids me show. \n",
            "\n",
            "DUKE OF AUMERLE: \n",
            "Remember, as thou read'st, thy promise pass'd: \n",
            "I do repent me; read not my name there \n",
            "My heart is not confederate with my hand. \n",
            "\n",
            "DUKE OF YORK: \n",
            "Speak on it, good York, and she will go. \n",
            "\n",
            "DUKE OF AUMERLE: \n",
            "Well, well, prepare him to my master's live \n",
            "\n",
            "First Keeper: \n",
            "We cannot choose without mine, Bohemia pleased \n",
            "In that thou hast struck more terror to the worse: \n",
            "Fell sorrow's tooth doth never rankle more \n",
            "Than when he bites, but lanceth not the sore. \n",
            "\n",
            "BENVOLIO: \n",
            "Come, come, away, under his grace! \n",
            "This is the news: ah, could'st thou fly! \n",
            "\n",
            "HENRY BOLINGBROKE: \n",
            "Good aunt, stand up. \n",
            "\n",
            "DUCHESS OF YORK: \n",
            "Nay, do believe me. \n",
            "\n",
            "DUKE OF YORK: \n",
            "Madam, I am a king? \n",
            "\n",
            "DUKE OF YORK: \n",
            "Make a unruly woman! \n",
            "\n",
            "LADY CAPULET: \n",
            "O what, I pray thee? \n",
            "\n",
            "ROMEO: \n",
            "For God's sake, fairly let her be entreated: \n",
            "Tell her I send to her my kind commends; \n",
            "Take special care help me, and my young prepare home, or now be eased \n",
            "With being nothing. Music do I hear? \n",
            "Ha, ha! do time: every man that you have took. \n",
            "\n",
            "ROMEO: \n",
            "Sin from thy lips? O trespass sweetly urged! \n",
            "Give me my sin again. \n",
            "\n",
            "JULIET: \n",
            "You kiss how true. \n",
            "\n",
            "PRINCE: \n",
            "O, who is that? \n",
            "\n",
            "Nurse: \n",
            "O woe! O woful, woful, woful day! \n",
            "Most lamentable day, most woful day, \n",
            "That ever, ever, I did yet behold! \n",
            "O day! O day! O day, I am your child, \n",
            "And none shall be to feed on one \n",
            "But durst you? wife! \n",
            "\n",
            "JULIET: \n",
            "The note lies in's throat, if he say I am not to myself \n",
            "And, in a skains-mates. And thou must hear by \n",
            "gage \n",
            "That know your best is well before you have \n",
            "done, \n",
            "For 'twas a time to be Petruchio's wife, \n",
            "As wealth or help, and you shall not \n",
            "\n",
            "BENVOLIO: \n",
            "O single-soled jest, solely singular for the \n",
            "singleness. \n",
            "\n",
            "MERCUTIO: \n",
            "Come between us, good Benvolio; my wits faint. \n",
            "\n",
            "ROMEO: \n",
            "Switch and spurs, switch and spurs; or I'll cry a match. \n",
            "\n",
            "MERCUTIO: \n",
            "Nay, if thy wits run the wild-goose chase, I have \n",
            "done, for thou hast some great child; \n",
            "Happier the man, whom favourable stars \n",
            "Was wont enjoy'd: so this her house, \n",
            "Were I shall still encounter Tybalt? \n",
            "\n",
            "BENVOLIO: \n",
            "Come, you shall be with you. \n",
            "\n",
            "CAPULET: \n",
            "More than you yet so I. \n",
            "\n",
            "KING RICHARD III: \n",
            "Well, let us lose your finger, that you might leave \n",
            "it, and I think to make her will be, \n",
            "Who shall be thought their harms by wailing them. \n",
            "Madam, your highness would have it: \n",
            "And sent a hearers that weeping O she cannot run, \n",
            "And not heaven and will be ruled by him. \n",
            "\n",
            "CAPULET: \n",
            "Send out an hour before her night, \n",
            "On whence are you? \n",
            "\n",
            "PARIS: \n",
            "My business is not joy and day is that? \n",
            "\n",
            "LADY CAPULET: \n",
            "Marry, that 'marry' is the very theme \n",
            "I came to talk of. Tell me, daughter Juliet, \n",
            "How stands your disposition to be married? \n",
            "\n",
            "JULIET: \n",
            "It is an heaven and do you kill you. \n",
            "\n",
            "PARIS: \n",
            "Go, sirrah, trudge about \n",
            "Through fair Verona; find those persons out \n",
            "Whose names are written there, and to them say, \n",
            "My heart is very pitiful and flexible; \n",
            "Thou stern, obdurate, flinty, rough, remorseless. \n",
            "Bids't thou me rage? why, now thou hast thy wish: \n",
            "Wouldst have me weep? why, now thou hast thy will: \n",
            "For raging wind blows up incessant showers, \n",
            "And when the rage allays, the rain begins. \n",
            "These tears are this sweet father to her will, \n",
            "That I'll be health; but, whilst I \n",
            "live, I think, in the thing you gave in fight, \n",
            "And in the fall, the gods never \n",
            "And so short house ne'er would be, sir, I myself \n",
            "Rescued the Black Prince, that young Mars of men, \n",
            "From forth the blood of heaven, I will be seen. \n",
            "Under the tents I'll play the housewife for this once. What, ho! \n",
            "They are all forth. Well, I will walk myself \n",
            "To County Paris, to prepare him up \n",
            "Against to-morrow: my heart is wondrous light, \n",
            "Since this same wayward girl is so reclaim'd. \n",
            "\n",
            "JULIET: \n",
            "Ay, those attires are best: but, gentle nurse, \n",
            "I pray thee, hence to London her, is not along, \n",
            "That stops my tongue, \n",
            "Will raise us this have done, be held \n",
            "By whom you should, as she takes her farewell of \n",
            "many ten times louder than the sea and next impatient sons. \n",
            "Unless the reason that I have even love a puissant host; \n",
            "And craves your company for speedy counsel. \n",
            "\n",
            "WARWICK: \n",
            "Why then it sorts, brave warriors, let's away. \n",
            "3 KING HENRY VI \n",
            "\n",
            "RUTLAND: \n",
            "Ah, whither shall I fly to 'scape their hands? \n",
            "Ah, tutor, look where bloody Clifford comes! \n",
            "\n",
            "CLIFFORD: \n",
            "Chaplain, away! thy priesthood saves thy life. \n",
            "As for the brat of this accursed duke, \n",
            "Whose father slew my father, he shall die. \n",
            "\n",
            "Tutor: \n",
            "Ah, York is gone in slaughter to be in his father's head. \n",
            "\n",
            "QUEEN MARGARET: \n",
            "Great lords, sweet Edward shall be back'd with France. \n",
            "\n",
            "HASTINGS: \n",
            "Lord Warwick, and a peers of France should smile at that. \n",
            "But for the rest, you bear the right. Constable, what say you to you, \n",
            "But what thou pleasure kill'd his majesty \n",
            "Unto the sister of your drooping lands, \n",
            "And stronger and than kings \n",
            "When the sun sets, who doth not do for service. \n",
            "\n",
            "Clown: \n",
            "Then fare you well: I may leave grazing, were I of your flock, \n",
            "And only live by gazing. \n",
            "\n",
            "PERDITA: \n",
            "Out, alas! \n",
            "You'd be so lean, that blasts of January \n",
            "Would blow you through and through. \n",
            "Now, my fair'st friend, \n",
            "I would I had some flowers o' the spring that "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLR_S72lPhuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}